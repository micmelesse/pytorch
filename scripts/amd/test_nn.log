test_to (__main__.PackedSequenceTest) ... ok
test_to_memory_format (__main__.PackedSequenceTest) ... ok
test_total_length (__main__.PackedSequenceTest) ... ok
test_type_casts (__main__.PackedSequenceTest)
Test type casting of `PackedSequence` against type casting of tensor ... ok
test_wrong_order (__main__.PackedSequenceTest) ... ok
test_add_relu (__main__.TestAddRelu) ... ok
test_avg_pool1d_ceil_mode (__main__.TestAvgPool) ... ok
test_avg_pool2d_ceil_mode (__main__.TestAvgPool) ... ok
test_avg_pool2d_with_zero_divisor (__main__.TestAvgPool) ... ok
test_avg_pool3d_ceil_mode (__main__.TestAvgPool) ... ok
test_avg_pool3d_with_zero_divisor (__main__.TestAvgPool) ... ok
test_doubletensor_avg_pool2d (__main__.TestAvgPool) ... ok
test_doubletensor_avg_pool2d_with_divisor (__main__.TestAvgPool) ... ok
test_doubletensor_avg_pool3d (__main__.TestAvgPool) ... ok
test_doubletensor_avg_pool3d_with_divisor (__main__.TestAvgPool) ... ok
test_constant_pad_nd (__main__.TestConstantPadNd) ... ok
test_preserves_memory_format (__main__.TestConstantPadNd) ... ok
test_pickle_softsign (__main__.TestFunctionalPickle) ... ok
test_fuse_module_eval_numerics (__main__.TestFusionEval) ... ok
test_chained_initialization (__main__.TestLazyModules) ... ok
test_invalid_functions (__main__.TestLazyModules) ... ok
test_lazy_batchnorm1d (__main__.TestLazyModules) ... /opt/conda/lib/python3.6/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
ok
test_lazy_batchnorm1d_pickle (__main__.TestLazyModules) ... ok
test_lazy_batchnorm1d_state (__main__.TestLazyModules) ... ok
test_lazy_batchnorm2d (__main__.TestLazyModules) ... ok
test_lazy_batchnorm2d_pickle (__main__.TestLazyModules) ... ok
test_lazy_batchnorm2d_state (__main__.TestLazyModules) ... ok
test_lazy_batchnorm3d (__main__.TestLazyModules) ... ok
test_lazy_batchnorm3d_pickle (__main__.TestLazyModules) ... ok
test_lazy_batchnorm3d_state (__main__.TestLazyModules) ... ok
test_lazy_conv1d (__main__.TestLazyModules) ... ok
test_lazy_conv1d_pickle (__main__.TestLazyModules) ... ok
test_lazy_conv1d_state (__main__.TestLazyModules) ... ok
test_lazy_conv2d (__main__.TestLazyModules) ... ok
test_lazy_conv2d_pickle (__main__.TestLazyModules) ... ok
test_lazy_conv2d_state (__main__.TestLazyModules) ... ok
test_lazy_conv3d (__main__.TestLazyModules) ... ok
test_lazy_conv3d_pickle (__main__.TestLazyModules) ... ok
test_lazy_conv3d_state (__main__.TestLazyModules) ... ok
test_lazy_conv_transpose1d_pickle (__main__.TestLazyModules) ... ok
test_lazy_conv_transpose1d_state (__main__.TestLazyModules) ... ok
test_lazy_conv_transpose2d (__main__.TestLazyModules) ... ok
test_lazy_conv_transpose2d_pickle (__main__.TestLazyModules) ... ok
test_lazy_conv_transpose2d_state (__main__.TestLazyModules) ... ok
test_lazy_conv_transpose3d (__main__.TestLazyModules) ... ok
test_lazy_conv_transpose3d_pickle (__main__.TestLazyModules) ... ok
test_lazy_conv_transpose3d_state (__main__.TestLazyModules) ... ok
test_lazy_conv_transposed1d (__main__.TestLazyModules) ... ok
test_lazy_linear_pickle (__main__.TestLazyModules) ... ok
test_lazy_module_buffer (__main__.TestLazyModules) ... ok
test_lazy_module_jit_buffer (__main__.TestLazyModules) ... ok
test_lazy_module_jit_param (__main__.TestLazyModules) ... ok
test_lazy_module_parameter (__main__.TestLazyModules) ... ok
test_lazy_share_memory_buffer (__main__.TestLazyModules) ... ok
test_lazy_share_memory_param (__main__.TestLazyModules) ... ok
test_linear (__main__.TestLazyModules) ... ok
test_linear_state (__main__.TestLazyModules) ... ok
test_materialize_device (__main__.TestLazyModules) ... ok
test_materialize_dtype (__main__.TestLazyModules) ... ok
test_optimizer_pass (__main__.TestLazyModules) ... ok
test_spectral_norm (__main__.TestLazyModules) ... ok
test_weight_norm (__main__.TestLazyModules) ... ok
test_global_and_local_hooks_order (__main__.TestModuleGlobalHooks) ... /opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py:938: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
ok
test_module_backward_global_hook_writeable (__main__.TestModuleGlobalHooks) ... ok
test_module_global_forward_preforward_hook_writeable (__main__.TestModuleGlobalHooks) ... ok
test_module_global_hook_invalid_outputs (__main__.TestModuleGlobalHooks) ... ok
test_module_global_hooks (__main__.TestModuleGlobalHooks) ... ok
test_AdaptiveAvgPool1d (__main__.TestNN) ... ok
test_AdaptiveAvgPool1d_cuda (__main__.TestNN) ... ok
test_AdaptiveAvgPool1d_one_output (__main__.TestNN) ... ok
test_AdaptiveAvgPool1d_one_output_cuda (__main__.TestNN) ... ok
test_AdaptiveAvgPool2d_alert_nondeterministic_cuda (__main__.TestNN) ... /opt/conda/lib/python3.6/site-packages/torch/__init__.py:429: UserWarning: torch.use_deterministic_algorithms is in beta, and its design and functionality may change in the future. (Triggered internally at  /root/pytorch/aten/src/ATen/Context.cpp:70.)
  _C._set_deterministic_algorithms(d)
ok
test_AdaptiveAvgPool2d_single (__main__.TestNN) ... ok
test_AdaptiveAvgPool2d_single_1x1output (__main__.TestNN) ... ok
test_AdaptiveAvgPool2d_single_1x1output_cuda (__main__.TestNN) ... ok
test_AdaptiveAvgPool2d_single_cuda (__main__.TestNN) ... ok
test_AdaptiveAvgPool2d_tuple (__main__.TestNN) ... ok
test_AdaptiveAvgPool2d_tuple_cuda (__main__.TestNN) ... ok
test_AdaptiveAvgPool2d_tuple_none (__main__.TestNN) ... ok
test_AdaptiveAvgPool2d_tuple_none_cuda (__main__.TestNN) ... ok
test_AdaptiveAvgPool3d_alert_nondeterministic_cuda (__main__.TestNN) ... ok
test_AdaptiveAvgPool3d_single (__main__.TestNN) ... ok
test_AdaptiveAvgPool3d_single_cuda (__main__.TestNN) ... ok
test_AdaptiveAvgPool3d_tuple (__main__.TestNN) ... ok
test_AdaptiveAvgPool3d_tuple_cuda (__main__.TestNN) ... ok
test_AdaptiveAvgPool3d_tuple_none (__main__.TestNN) ... ok
test_AdaptiveAvgPool3d_tuple_none_cuda (__main__.TestNN) ... ok
test_AdaptiveLogSoftmax (__main__.TestNN) ... ok
test_AdaptiveLogSoftmax_cuda (__main__.TestNN) ... ok
test_AdaptiveMaxPool1d (__main__.TestNN) ... ok
test_AdaptiveMaxPool1d_cuda (__main__.TestNN) ... ok
test_AdaptiveMaxPool2d_alert_nondeterministic_cuda (__main__.TestNN) ... ok
test_AdaptiveMaxPool2d_single (__main__.TestNN) ... ok
test_AdaptiveMaxPool2d_single_cuda (__main__.TestNN) ... ok
test_AdaptiveMaxPool2d_tuple (__main__.TestNN) ... ok
test_AdaptiveMaxPool2d_tuple_cuda (__main__.TestNN) ... ok
test_AdaptiveMaxPool2d_tuple_none (__main__.TestNN) ... ok
test_AdaptiveMaxPool2d_tuple_none_cuda (__main__.TestNN) ... ok
test_AdaptiveMaxPool3d_single (__main__.TestNN) ... ok
test_AdaptiveMaxPool3d_single_cuda (__main__.TestNN) ... ok
test_AdaptiveMaxPool3d_single_nonatomic (__main__.TestNN) ... ok
test_AdaptiveMaxPool3d_single_nonatomic_cuda (__main__.TestNN) ... ok
test_AdaptiveMaxPool3d_tuple (__main__.TestNN) ... ok
test_AdaptiveMaxPool3d_tuple_cuda (__main__.TestNN) ... ok
test_AdaptiveMaxPool3d_tuple_nonatomic (__main__.TestNN) ... ok
test_AdaptiveMaxPool3d_tuple_nonatomic_cuda (__main__.TestNN) ... ok
test_AdaptiveMaxPool3d_tuple_none (__main__.TestNN) ... ok
test_AdaptiveMaxPool3d_tuple_none_cuda (__main__.TestNN) ... ok
test_AlphaDropout (__main__.TestNN) ... ok
test_AvgPool1d (__main__.TestNN) ... ok
test_AvgPool1d_cuda (__main__.TestNN) ... ok
test_AvgPool1d_stride (__main__.TestNN) ... ok
test_AvgPool1d_stride_cuda (__main__.TestNN) ... ok
test_AvgPool1d_stride_pad (__main__.TestNN) ... ok
test_AvgPool1d_stride_pad_cuda (__main__.TestNN) ... ok
test_AvgPool2d (__main__.TestNN) ... ok
test_AvgPool2d_cuda (__main__.TestNN) ... ok
test_AvgPool2d_divisor (__main__.TestNN) ... ok
test_AvgPool2d_divisor_cuda (__main__.TestNN) ... ok
test_AvgPool2d_divisor_stride (__main__.TestNN) ... ok
test_AvgPool2d_divisor_stride_cuda (__main__.TestNN) ... ok
test_AvgPool2d_divisor_stride_pad (__main__.TestNN) ... ok
test_AvgPool2d_divisor_stride_pad_cuda (__main__.TestNN) ... ok
test_AvgPool2d_divisor_stride_pad_with_long_tensor (__main__.TestNN) ... ok
test_AvgPool2d_divisor_stride_pad_with_long_tensor_cuda (__main__.TestNN) ... skipped 'Excluded from CUDA tests'
test_AvgPool2d_divisor_stride_with_long_tensor (__main__.TestNN) ... ok
test_AvgPool2d_divisor_stride_with_long_tensor_cuda (__main__.TestNN) ... skipped 'Excluded from CUDA tests'
test_AvgPool2d_divisor_with_long_tensor (__main__.TestNN) ... ok
test_AvgPool2d_divisor_with_long_tensor_cuda (__main__.TestNN) ... skipped 'Excluded from CUDA tests'
test_AvgPool2d_stride (__main__.TestNN) ... ok
test_AvgPool2d_stride_cuda (__main__.TestNN) ... ok
test_AvgPool2d_stride_pad (__main__.TestNN) ... ok
test_AvgPool2d_stride_pad_cuda (__main__.TestNN) ... ok
test_AvgPool3d (__main__.TestNN) ... ok
test_AvgPool3d_alert_nondeterministic_cuda (__main__.TestNN) ... ok
test_AvgPool3d_cuda (__main__.TestNN) ... ok
test_AvgPool3d_divisor (__main__.TestNN) ... ok
test_AvgPool3d_divisor_cuda (__main__.TestNN) ... ok
test_AvgPool3d_divisor_stride (__main__.TestNN) ... ok
test_AvgPool3d_divisor_stride1_pad0_gpu_input (__main__.TestNN) ... ok
test_AvgPool3d_divisor_stride1_pad0_gpu_input_cuda (__main__.TestNN) ... ok
test_AvgPool3d_divisor_stride1_pad0_gpu_input_with_long_tensor (__main__.TestNN) ... ok
test_AvgPool3d_divisor_stride1_pad0_gpu_input_with_long_tensor_cuda (__main__.TestNN) ... skipped 'Excluded from CUDA tests'
test_AvgPool3d_divisor_stride_cuda (__main__.TestNN) ... ok
test_AvgPool3d_divisor_stride_pad (__main__.TestNN) ... ok
test_AvgPool3d_divisor_stride_pad_cuda (__main__.TestNN) ... ok
test_AvgPool3d_divisor_stride_pad_gpu_fixedkw_output (__main__.TestNN) ... ok
test_AvgPool3d_divisor_stride_pad_gpu_fixedkw_output_cuda (__main__.TestNN) ... ok
test_AvgPool3d_divisor_stride_pad_gpu_fixedkw_output_with_long_tensor (__main__.TestNN) ... ok
test_AvgPool3d_divisor_stride_pad_gpu_fixedkw_output_with_long_tensor_cuda (__main__.TestNN) ... skipped 'Excluded from CUDA tests'
test_AvgPool3d_divisor_stride_pad_gpu_general_output (__main__.TestNN) ... ok
test_AvgPool3d_divisor_stride_pad_gpu_general_output_cuda (__main__.TestNN) ... ok
test_AvgPool3d_divisor_stride_pad_gpu_general_output_with_long_tensor (__main__.TestNN) ... ok
test_AvgPool3d_divisor_stride_pad_gpu_general_output_with_long_tensor_cuda (__main__.TestNN) ... skipped 'Excluded from CUDA tests'
test_AvgPool3d_divisor_stride_pad_gpu_input_nooverlap (__main__.TestNN) ... ok
test_AvgPool3d_divisor_stride_pad_gpu_input_nooverlap_cuda (__main__.TestNN) ... ok
test_AvgPool3d_divisor_stride_pad_gpu_input_nooverlap_with_long_tensor (__main__.TestNN) ... ok
test_AvgPool3d_divisor_stride_pad_gpu_input_nooverlap_with_long_tensor_cuda (__main__.TestNN) ... skipped 'Excluded from CUDA tests'
test_AvgPool3d_divisor_stride_pad_with_long_tensor (__main__.TestNN) ... ok
test_AvgPool3d_divisor_stride_pad_with_long_tensor_cuda (__main__.TestNN) ... skipped 'Excluded from CUDA tests'
test_AvgPool3d_divisor_stride_with_long_tensor (__main__.TestNN) ... ok
test_AvgPool3d_divisor_stride_with_long_tensor_cuda (__main__.TestNN) ... skipped 'Excluded from CUDA tests'
test_AvgPool3d_divisor_with_long_tensor (__main__.TestNN) ... ok
test_AvgPool3d_divisor_with_long_tensor_cuda (__main__.TestNN) ... skipped 'Excluded from CUDA tests'
test_AvgPool3d_stride (__main__.TestNN) ... ok
test_AvgPool3d_stride1_pad0_gpu_input (__main__.TestNN) ... ok
test_AvgPool3d_stride1_pad0_gpu_input_cuda (__main__.TestNN) ... ok
test_AvgPool3d_stride_cuda (__main__.TestNN) ... ok
test_AvgPool3d_stride_pad (__main__.TestNN) ... ok
test_AvgPool3d_stride_pad_cuda (__main__.TestNN) ... ok
test_AvgPool3d_stride_pad_gpu_fixedkw_output (__main__.TestNN) ... ok
test_AvgPool3d_stride_pad_gpu_fixedkw_output_cuda (__main__.TestNN) ... ok
test_AvgPool3d_stride_pad_gpu_general_output (__main__.TestNN) ... ok
test_AvgPool3d_stride_pad_gpu_general_output_cuda (__main__.TestNN) ... ok
test_AvgPool3d_stride_pad_gpu_input_nooverlap (__main__.TestNN) ... ok
test_AvgPool3d_stride_pad_gpu_input_nooverlap_cuda (__main__.TestNN) ... ok
test_BCELoss (__main__.TestNN) ... ok
test_BCELoss_cuda_bfloat16 (__main__.TestNN) ... ok
test_BCELoss_cuda_double (__main__.TestNN) ... ok
test_BCELoss_cuda_float (__main__.TestNN) ... ok
test_BCELoss_cuda_half (__main__.TestNN) ... ok
test_BCELoss_no_reduce (__main__.TestNN) ... ok
test_BCELoss_no_reduce_cuda (__main__.TestNN) ... ok
test_BCELoss_no_reduce_scalar (__main__.TestNN) ... ok
test_BCELoss_no_reduce_scalar_cuda (__main__.TestNN) ... ok
test_BCELoss_scalar_weights (__main__.TestNN) ... ok
test_BCELoss_scalar_weights_cuda_bfloat16 (__main__.TestNN) ... ok
test_BCELoss_scalar_weights_cuda_double (__main__.TestNN) ... ok
test_BCELoss_scalar_weights_cuda_float (__main__.TestNN) ... ok
test_BCELoss_scalar_weights_cuda_half (__main__.TestNN) ... ok
test_BCELoss_weights (__main__.TestNN) ... ok
test_BCELoss_weights_cuda_bfloat16 (__main__.TestNN) ... ok
test_BCELoss_weights_cuda_double (__main__.TestNN) ... ok
test_BCELoss_weights_cuda_float (__main__.TestNN) ... ok
test_BCELoss_weights_cuda_half (__main__.TestNN) ... ok
test_BCELoss_weights_no_reduce (__main__.TestNN) ... ok
test_BCELoss_weights_no_reduce_cuda (__main__.TestNN) ... ok
test_BCELoss_weights_no_reduce_scalar (__main__.TestNN) ... ok
test_BCELoss_weights_no_reduce_scalar_cuda (__main__.TestNN) ... ok
test_BCEWithLogitsLoss (__main__.TestNN) ... ok
test_BCEWithLogitsLoss_cuda_double (__main__.TestNN) ... ok
test_BCEWithLogitsLoss_cuda_float (__main__.TestNN) ... ok
test_BCEWithLogitsLoss_cuda_half (__main__.TestNN) ... ok
test_BCEWithLogitsLoss_legacy_enum (__main__.TestNN) ... /opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
ok
test_BCEWithLogitsLoss_legacy_enum_cuda (__main__.TestNN) ... ok
test_BCEWithLogitsLoss_no_reduce (__main__.TestNN) ... ok
test_BCEWithLogitsLoss_no_reduce_cuda (__main__.TestNN) ... ok
test_BCEWithLogitsLoss_no_reduce_scalar (__main__.TestNN) ... ok
test_BCEWithLogitsLoss_no_reduce_scalar_cuda (__main__.TestNN) ... ok
test_BCEWithLogitsLoss_scalar_weights (__main__.TestNN) ... ok
test_BCEWithLogitsLoss_scalar_weights_cuda_double (__main__.TestNN) ... ok
test_BCEWithLogitsLoss_scalar_weights_cuda_float (__main__.TestNN) ... ok
test_BCEWithLogitsLoss_scalar_weights_cuda_half (__main__.TestNN) ... ok
test_BCEWithLogitsLoss_weights (__main__.TestNN) ... ok
test_BCEWithLogitsLoss_weights_cuda_double (__main__.TestNN) ... ok
test_BCEWithLogitsLoss_weights_cuda_float (__main__.TestNN) ... ok
test_BCEWithLogitsLoss_weights_cuda_half (__main__.TestNN) ... ok
test_BatchNorm1d_3d_input (__main__.TestNN) ... [W Module.cpp:506] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
ok
test_BatchNorm1d_3d_input_cuda (__main__.TestNN) ... ok
test_BatchNorm1d_3d_input_eval (__main__.TestNN) ... ok
test_BatchNorm1d_3d_input_eval_cuda (__main__.TestNN) ... ok
test_BatchNorm1d_3d_input_not_affine (__main__.TestNN) ... ok
test_BatchNorm1d_3d_input_not_affine_cuda (__main__.TestNN) ... ok
test_BatchNorm1d_3d_input_not_affine_eval (__main__.TestNN) ... ok
test_BatchNorm1d_3d_input_not_affine_eval_cuda (__main__.TestNN) ... ok
test_BatchNorm1d_affine (__main__.TestNN) ... ok
test_BatchNorm1d_affine_cuda (__main__.TestNN) ... ok
test_BatchNorm1d_affine_eval (__main__.TestNN) ... ok
test_BatchNorm1d_affine_eval_cuda (__main__.TestNN) ... ok
test_BatchNorm1d_affine_simple_average (__main__.TestNN) ... ok
test_BatchNorm1d_affine_simple_average_cuda (__main__.TestNN) ... ok
test_BatchNorm1d_affine_simple_average_eval (__main__.TestNN) ... ok
test_BatchNorm1d_affine_simple_average_eval_cuda (__main__.TestNN) ... ok
test_BatchNorm1d_not_affine (__main__.TestNN) ... ok
test_BatchNorm1d_not_affine_cuda (__main__.TestNN) ... ok
test_BatchNorm1d_not_affine_eval (__main__.TestNN) ... ok
test_BatchNorm1d_not_affine_eval_cuda (__main__.TestNN) ... ok
test_BatchNorm1d_not_tracking_stats (__main__.TestNN) ... ok
test_BatchNorm1d_not_tracking_stats_cuda (__main__.TestNN) ... ok
test_BatchNorm1d_not_tracking_stats_eval (__main__.TestNN) ... ok
test_BatchNorm1d_not_tracking_stats_eval_cuda (__main__.TestNN) ... ok
test_BatchNorm1d_zero_batch (__main__.TestNN) ... ok
test_BatchNorm1d_zero_batch_cuda (__main__.TestNN) ... ok
test_BatchNorm1d_zero_batch_eval (__main__.TestNN) ... ok
test_BatchNorm1d_zero_batch_eval_cuda (__main__.TestNN) ... ok
test_BatchNorm2d (__main__.TestNN) ... ok
test_BatchNorm2d_2d_simple_average (__main__.TestNN) ... ok
test_BatchNorm2d_2d_simple_average_cuda (__main__.TestNN) ... ok
test_BatchNorm2d_2d_simple_average_eval (__main__.TestNN) ... ok
test_BatchNorm2d_2d_simple_average_eval_cuda (__main__.TestNN) ... ok
test_BatchNorm2d_cuda (__main__.TestNN) ... ok
test_BatchNorm2d_eval (__main__.TestNN) ... ok
test_BatchNorm2d_eval_cuda (__main__.TestNN) ... ok
test_BatchNorm2d_momentum (__main__.TestNN) ... ok
test_BatchNorm2d_momentum_cuda (__main__.TestNN) ... ok
test_BatchNorm2d_momentum_eval (__main__.TestNN) ... ok
test_BatchNorm2d_momentum_eval_cuda (__main__.TestNN) ... ok
test_BatchNorm2d_not_affine (__main__.TestNN) ... ok
test_BatchNorm2d_not_affine_cuda (__main__.TestNN) ... ok
test_BatchNorm2d_not_affine_eval (__main__.TestNN) ... ok
test_BatchNorm2d_not_affine_eval_cuda (__main__.TestNN) ... ok
test_BatchNorm2d_not_tracking_stats (__main__.TestNN) ... ok
test_BatchNorm2d_not_tracking_stats_cuda (__main__.TestNN) ... ok
test_BatchNorm2d_not_tracking_stats_eval (__main__.TestNN) ... ok
test_BatchNorm2d_not_tracking_stats_eval_cuda (__main__.TestNN) ... ok
test_BatchNorm2d_zero_batch (__main__.TestNN) ... ok
test_BatchNorm2d_zero_batch_cuda (__main__.TestNN) ... ok
test_BatchNorm2d_zero_batch_eval (__main__.TestNN) ... ok
test_BatchNorm2d_zero_batch_eval_cuda (__main__.TestNN) ... ok
test_BatchNorm3d (__main__.TestNN) ... ok
test_BatchNorm3d_3d_simple_average (__main__.TestNN) ... ok
test_BatchNorm3d_3d_simple_average_cuda (__main__.TestNN) ... ok
test_BatchNorm3d_3d_simple_average_eval (__main__.TestNN) ... ok
test_BatchNorm3d_3d_simple_average_eval_cuda (__main__.TestNN) ... ok
test_BatchNorm3d_cuda (__main__.TestNN) ... ok
test_BatchNorm3d_eval (__main__.TestNN) ... ok
test_BatchNorm3d_eval_cuda (__main__.TestNN) ... ok
test_BatchNorm3d_momentum (__main__.TestNN) ... ok
test_BatchNorm3d_momentum_cuda (__main__.TestNN) ... ok
test_BatchNorm3d_momentum_eval (__main__.TestNN) ... ok
test_BatchNorm3d_momentum_eval_cuda (__main__.TestNN) ... ok
test_BatchNorm3d_not_affine (__main__.TestNN) ... ok
test_BatchNorm3d_not_affine_cuda (__main__.TestNN) ... ok
test_BatchNorm3d_not_affine_eval (__main__.TestNN) ... ok
test_BatchNorm3d_not_affine_eval_cuda (__main__.TestNN) ... ok
test_BatchNorm3d_not_tracking_stats (__main__.TestNN) ... ok
test_BatchNorm3d_not_tracking_stats_cuda (__main__.TestNN) ... ok
test_BatchNorm3d_not_tracking_stats_eval (__main__.TestNN) ... ok
test_BatchNorm3d_not_tracking_stats_eval_cuda (__main__.TestNN) ... ok
test_BatchNorm3d_zero_batch (__main__.TestNN) ... ok
test_BatchNorm3d_zero_batch_cuda (__main__.TestNN) ... ok
test_BatchNorm3d_zero_batch_eval (__main__.TestNN) ... ok
test_BatchNorm3d_zero_batch_eval_cuda (__main__.TestNN) ... ok
test_CELU (__main__.TestNN) ... ok
test_CELU_cuda (__main__.TestNN) ... ok
test_CELU_scalar (__main__.TestNN) ... ok
test_CELU_scalar_cuda (__main__.TestNN) ... ok
test_CTCLoss_2d_int_target_lengths_intlists (__main__.TestNN) ... ok
test_CTCLoss_2d_int_target_lengths_intlists_cuda_double (__main__.TestNN) ... ok
test_CTCLoss_2d_int_target_lengths_intlists_cuda_float (__main__.TestNN) ... ok
test_CTCLoss_2d_int_target_lengths_intlists_sum_reduction (__main__.TestNN) ... ok
test_CTCLoss_2d_int_target_lengths_intlists_sum_reduction_cuda_double (__main__.TestNN) ... ok
test_CTCLoss_2d_int_target_lengths_intlists_sum_reduction_cuda_float (__main__.TestNN) ... ok
test_CTCLoss_2d_int_target_lengths_tensors (__main__.TestNN) ... ok
test_CTCLoss_2d_int_target_lengths_tensors_cuda_double (__main__.TestNN) ... ok
test_CTCLoss_2d_int_target_lengths_tensors_cuda_float (__main__.TestNN) ... ok
test_CTCLoss_2d_int_target_lengths_tensors_sum_reduction (__main__.TestNN) ... ok
test_CTCLoss_2d_int_target_lengths_tensors_sum_reduction_cuda_double (__main__.TestNN) ... ok
test_CTCLoss_2d_int_target_lengths_tensors_sum_reduction_cuda_float (__main__.TestNN) ... ok
test_CTCLoss_2d_lengths_tensors (__main__.TestNN) ... ok
test_CTCLoss_2d_lengths_tensors_cuda_double (__main__.TestNN) ... ok
test_CTCLoss_2d_lengths_tensors_cuda_float (__main__.TestNN) ... ok
test_CTCLoss_2d_lengths_tensors_sum_reduction (__main__.TestNN) ... ok
test_CTCLoss_2d_lengths_tensors_sum_reduction_cuda_double (__main__.TestNN) ... ok
test_CTCLoss_2d_lengths_tensors_sum_reduction_cuda_float (__main__.TestNN) ... ok
test_CTCLoss_alert_nondeterministic_cuda_double (__main__.TestNN) ... ok
test_CTCLoss_alert_nondeterministic_cuda_float (__main__.TestNN) ... ok
test_CTCLoss_alert_nondeterministic_sum_reduction_cuda_double (__main__.TestNN) ... ok
test_CTCLoss_alert_nondeterministic_sum_reduction_cuda_float (__main__.TestNN) ... ok
test_CTCLoss_critical_target_len (__main__.TestNN) ... ok
test_CTCLoss_lengthchecks_cpu (__main__.TestNN) ... ok
test_CTCLoss_lengthchecks_cuda (__main__.TestNN) ... ok
test_CTCLoss_lengths_intlists (__main__.TestNN) ... ok
test_CTCLoss_lengths_intlists_cuda_double (__main__.TestNN) ... ok
test_CTCLoss_lengths_intlists_cuda_float (__main__.TestNN) ... ok
test_CTCLoss_lengths_intlists_sum_reduction (__main__.TestNN) ... ok
test_CTCLoss_lengths_intlists_sum_reduction_cuda_double (__main__.TestNN) ... ok
test_CTCLoss_lengths_intlists_sum_reduction_cuda_float (__main__.TestNN) ... ok
test_CTCLoss_lengths_tensors (__main__.TestNN) ... ok
test_CTCLoss_lengths_tensors_cuda_double (__main__.TestNN) ... ok
test_CTCLoss_lengths_tensors_cuda_float (__main__.TestNN) ... ok
test_CTCLoss_lengths_tensors_sum_reduction (__main__.TestNN) ... ok
test_CTCLoss_lengths_tensors_sum_reduction_cuda_double (__main__.TestNN) ... ok
test_CTCLoss_lengths_tensors_sum_reduction_cuda_float (__main__.TestNN) ... ok
test_CTCLoss_long_targets (__main__.TestNN) ... ok
test_CTCLoss_typechecks (__main__.TestNN) ... ok
test_CTCLoss_zero_infinity (__main__.TestNN) ... ok
test_ConstantPad1d (__main__.TestNN) ... ok
test_ConstantPad1d_complex (__main__.TestNN) ... ok
test_ConstantPad1d_complex_cuda (__main__.TestNN) ... ok
test_ConstantPad1d_cuda (__main__.TestNN) ... ok
test_ConstantPad2d (__main__.TestNN) ... ok
test_ConstantPad2d_complex (__main__.TestNN) ... ok
test_ConstantPad2d_complex_cuda (__main__.TestNN) ... ok
test_ConstantPad2d_cuda (__main__.TestNN) ... ok
test_ConstantPad3d (__main__.TestNN) ... ok
test_ConstantPad3d_complex (__main__.TestNN) ... ok
test_ConstantPad3d_complex_cuda (__main__.TestNN) ... ok
test_ConstantPad3d_cuda (__main__.TestNN) ... ok
test_Conv1d (__main__.TestNN) ... ok
test_Conv1d_circular_stride2_pad2 (__main__.TestNN) ... ok
test_Conv1d_circular_stride2_pad2_cuda (__main__.TestNN) ... ok
test_Conv1d_cuda (__main__.TestNN) ... ok
test_Conv1d_dilated (__main__.TestNN) ... ok
test_Conv1d_dilated_cuda (__main__.TestNN) ... ok
test_Conv1d_groups (__main__.TestNN) ... ok
test_Conv1d_groups_cuda (__main__.TestNN) ... ok
test_Conv1d_pad1 (__main__.TestNN) ... ok
test_Conv1d_pad1_cuda (__main__.TestNN) ... ok
test_Conv1d_pad1size1 (__main__.TestNN) ... ok
test_Conv1d_pad1size1_cuda (__main__.TestNN) ... ok
test_Conv1d_pad2 (__main__.TestNN) ... ok
test_Conv1d_pad2_cuda (__main__.TestNN) ... ok
test_Conv1d_pad2size1 (__main__.TestNN) ... ok
test_Conv1d_pad2size1_cuda (__main__.TestNN) ... ok
test_Conv1d_reflect_stride2_pad2 (__main__.TestNN) ... ok
test_Conv1d_reflect_stride2_pad2_cuda (__main__.TestNN) ... ok
test_Conv1d_replicate_stride2_pad2 (__main__.TestNN) ... ok
test_Conv1d_replicate_stride2_pad2_cuda (__main__.TestNN) ... ok
test_Conv1d_stride (__main__.TestNN) ... ok
test_Conv1d_stride_cuda (__main__.TestNN) ... ok
test_Conv1d_zero_batch (__main__.TestNN) ... ok
test_Conv1d_zero_batch_cuda (__main__.TestNN) ... ok
test_Conv1d_zeros_stride2_pad2 (__main__.TestNN) ... ok
test_Conv1d_zeros_stride2_pad2_cuda (__main__.TestNN) ... ok
test_Conv2d (__main__.TestNN) ... ok
test_Conv2d_1x1 (__main__.TestNN) ... ok
test_Conv2d_OneDNN (__main__.TestNN) ... ok
test_Conv2d_backward_twice (__main__.TestNN) ... ok
test_Conv2d_circular_stride2_pad2 (__main__.TestNN) ... ok
test_Conv2d_circular_stride2_pad2_cuda (__main__.TestNN) ... ok
test_Conv2d_cuda (__main__.TestNN) ... ok
test_Conv2d_depthwise (__main__.TestNN) ... ok
test_Conv2d_depthwise_cuda (__main__.TestNN) ... ok
test_Conv2d_depthwise_dilated (__main__.TestNN) ... ok
test_Conv2d_depthwise_dilated_cuda (__main__.TestNN) ... ok
test_Conv2d_depthwise_naive_groups_cuda (__main__.TestNN) ... ok
test_Conv2d_depthwise_padded (__main__.TestNN) ... ok
test_Conv2d_depthwise_padded_cuda (__main__.TestNN) ... ok
test_Conv2d_depthwise_strided (__main__.TestNN) ... ok
test_Conv2d_depthwise_strided_cuda (__main__.TestNN) ... ok
test_Conv2d_depthwise_with_multiplier (__main__.TestNN) ... ok
test_Conv2d_depthwise_with_multiplier_cuda (__main__.TestNN) ... ok
test_Conv2d_deterministic_cudnn (__main__.TestNN) ... ok
test_Conv2d_dilated (__main__.TestNN) ... ok
test_Conv2d_dilated_cuda (__main__.TestNN) ... ok
test_Conv2d_dilated_with_long_tensor (__main__.TestNN) ... ok
test_Conv2d_dilated_with_long_tensor_cuda (__main__.TestNN) ... skipped 'Excluded from CUDA tests'
test_Conv2d_groups (__main__.TestNN) ... ok
test_Conv2d_groups_cuda (__main__.TestNN) ... ok
test_Conv2d_groups_nobias (__main__.TestNN) ... ok
test_Conv2d_groups_nobias_v2 (__main__.TestNN) ... ok
test_Conv2d_groups_thnn (__main__.TestNN) ... ok
test_Conv2d_groups_thnn_cuda (__main__.TestNN) ... ok
test_Conv2d_groups_thnn_with_long_tensor (__main__.TestNN) ... ok
test_Conv2d_groups_thnn_with_long_tensor_cuda (__main__.TestNN) ... skipped 'Excluded from CUDA tests'
test_Conv2d_groups_with_long_tensor (__main__.TestNN) ... ok
test_Conv2d_groups_with_long_tensor_cuda (__main__.TestNN) ... skipped 'Excluded from CUDA tests'
test_Conv2d_inconsistent_types (__main__.TestNN) ... ok
test_Conv2d_inconsistent_types_on_GPU_with_cudnn (__main__.TestNN) ... ok
test_Conv2d_inconsistent_types_on_GPU_without_cudnn (__main__.TestNN) ... ok
test_Conv2d_large_workspace (__main__.TestNN) ... ok
test_Conv2d_missing_argument (__main__.TestNN) ... ok
test_Conv2d_no_bias (__main__.TestNN) ... ok
test_Conv2d_no_bias_cuda (__main__.TestNN) ... ok
test_Conv2d_no_bias_with_long_tensor (__main__.TestNN) ... ok
test_Conv2d_no_bias_with_long_tensor_cuda (__main__.TestNN) ... skipped 'Excluded from CUDA tests'
test_Conv2d_padding (__main__.TestNN) ... ok
test_Conv2d_padding_cuda (__main__.TestNN) ... ok
test_Conv2d_padding_with_long_tensor (__main__.TestNN) ... ok
test_Conv2d_padding_with_long_tensor_cuda (__main__.TestNN) ... skipped 'Excluded from CUDA tests'
test_Conv2d_reflect_stride2_pad2 (__main__.TestNN) ... ok
test_Conv2d_reflect_stride2_pad2_cuda (__main__.TestNN) ... ok
test_Conv2d_replicate_stride2_pad2 (__main__.TestNN) ... ok
test_Conv2d_replicate_stride2_pad2_cuda (__main__.TestNN) ... ok
test_Conv2d_strided (__main__.TestNN) ... ok
test_Conv2d_strided_cuda (__main__.TestNN) ... ok
test_Conv2d_strided_with_long_tensor (__main__.TestNN) ... ok
test_Conv2d_strided_with_long_tensor_cuda (__main__.TestNN) ... skipped 'Excluded from CUDA tests'
test_Conv2d_with_long_tensor (__main__.TestNN) ... ok
test_Conv2d_with_long_tensor_cuda (__main__.TestNN) ... skipped 'Excluded from CUDA tests'
test_Conv2d_zero_batch (__main__.TestNN) ... ok
test_Conv2d_zero_batch_cuda (__main__.TestNN) ... ok
test_Conv2d_zero_batch_with_long_tensor (__main__.TestNN) ... ok
test_Conv2d_zero_batch_with_long_tensor_cuda (__main__.TestNN) ... skipped 'Excluded from CUDA tests'
test_Conv2d_zeros_stride2_pad2 (__main__.TestNN) ... ok
test_Conv2d_zeros_stride2_pad2_cuda (__main__.TestNN) ... ok
test_Conv3d (__main__.TestNN) ... ok
test_Conv3d_1x1x1_no_bias (__main__.TestNN) ... ok
test_Conv3d_1x1x1_no_bias_cuda (__main__.TestNN) ... ok
test_Conv3d_1x1x1_no_bias_with_long_tensor (__main__.TestNN) ... ok
test_Conv3d_1x1x1_no_bias_with_long_tensor_cuda (__main__.TestNN) ... skipped 'Excluded from CUDA tests'
test_Conv3d_circular_stride2_pad2 (__main__.TestNN) ... ok
test_Conv3d_circular_stride2_pad2_cuda (__main__.TestNN) ... ok
test_Conv3d_cuda (__main__.TestNN) ... ok
test_Conv3d_depthwise_naive_groups_cuda (__main__.TestNN) ... ok
test_Conv3d_dilated (__main__.TestNN) ... ok
test_Conv3d_dilated_cuda (__main__.TestNN) ... ok
test_Conv3d_dilated_strided (__main__.TestNN) ... ok
test_Conv3d_dilated_strided_cuda (__main__.TestNN) ... ok
test_Conv3d_groups (__main__.TestNN) ... ok
test_Conv3d_groups_cuda (__main__.TestNN) ... ok
test_Conv3d_groups_nobias (__main__.TestNN) ... ok
test_Conv3d_groups_wbias (__main__.TestNN) ... ok
test_Conv3d_groups_with_long_tensor (__main__.TestNN) ... ok
test_Conv3d_groups_with_long_tensor_cuda (__main__.TestNN) ... skipped 'Excluded from CUDA tests'
test_Conv3d_no_bias (__main__.TestNN) ... ok
test_Conv3d_no_bias_cuda (__main__.TestNN) ... ok
test_Conv3d_no_bias_with_long_tensor (__main__.TestNN) ... ok
test_Conv3d_no_bias_with_long_tensor_cuda (__main__.TestNN) ... skipped 'Excluded from CUDA tests'
test_Conv3d_replicate_stride2_pad2 (__main__.TestNN) ... ok
test_Conv3d_replicate_stride2_pad2_cuda (__main__.TestNN) ... ok
test_Conv3d_stride (__main__.TestNN) ... ok
test_Conv3d_stride_cuda (__main__.TestNN) ... ok
test_Conv3d_stride_padding (__main__.TestNN) ... ok
test_Conv3d_stride_padding_cuda (__main__.TestNN) ... ok
test_Conv3d_stride_padding_with_long_tensor (__main__.TestNN) ... ok
test_Conv3d_stride_padding_with_long_tensor_cuda (__main__.TestNN) ... skipped 'Excluded from CUDA tests'
test_Conv3d_stride_with_long_tensor (__main__.TestNN) ... ok
test_Conv3d_stride_with_long_tensor_cuda (__main__.TestNN) ... skipped 'Excluded from CUDA tests'
test_Conv3d_with_long_tensor (__main__.TestNN) ... ok
test_Conv3d_with_long_tensor_cuda (__main__.TestNN) ... skipped 'Excluded from CUDA tests'
test_Conv3d_zero_batch (__main__.TestNN) ... ok
test_Conv3d_zero_batch_cuda (__main__.TestNN) ... ok
test_Conv3d_zero_batch_with_long_tensor (__main__.TestNN) ... ok
test_Conv3d_zero_batch_with_long_tensor_cuda (__main__.TestNN) ... skipped 'Excluded from CUDA tests'
test_Conv3d_zeros_stride2_pad2 (__main__.TestNN) ... ok
test_Conv3d_zeros_stride2_pad2_cuda (__main__.TestNN) ... ok
test_ConvTranspose1d (__main__.TestNN) ... ok
test_ConvTranspose1d_cuda (__main__.TestNN) ... ok
test_ConvTranspose1d_dilated (__main__.TestNN) ... ok
test_ConvTranspose1d_dilated_cuda (__main__.TestNN) ... ok
test_ConvTranspose1d_groups (__main__.TestNN) ... ok
test_ConvTranspose1d_groups_cuda (__main__.TestNN) ... ok
test_ConvTranspose1d_no_bias (__main__.TestNN) ... ok
test_ConvTranspose1d_no_bias_cuda (__main__.TestNN) ... ok
test_ConvTranspose2d (__main__.TestNN) ... ok
test_ConvTranspose2d_cuda (__main__.TestNN) ... ok
test_ConvTranspose2d_dilated (__main__.TestNN) ... ok
test_ConvTranspose2d_dilated_cuda (__main__.TestNN) ... ok
test_ConvTranspose2d_dilated_with_long_tensor (__main__.TestNN) ... ok
test_ConvTranspose2d_dilated_with_long_tensor_cuda (__main__.TestNN) ... skipped 'Excluded from CUDA tests'
test_ConvTranspose2d_groups (__main__.TestNN) ... ok
test_ConvTranspose2d_groups_cuda (__main__.TestNN) ... ok
test_ConvTranspose2d_groups_with_long_tensor (__main__.TestNN) ... ok
test_ConvTranspose2d_groups_with_long_tensor_cuda (__main__.TestNN) ... skipped 'Excluded from CUDA tests'
test_ConvTranspose2d_half_cublas_gemm (__main__.TestNN) ... ok
test_ConvTranspose2d_large_output_padding (__main__.TestNN) ... ok
test_ConvTranspose2d_no_bias (__main__.TestNN) ... ok
test_ConvTranspose2d_no_bias_cuda (__main__.TestNN) ... ok
test_ConvTranspose2d_no_bias_with_long_tensor (__main__.TestNN) ... ok
test_ConvTranspose2d_no_bias_with_long_tensor_cuda (__main__.TestNN) ... skipped 'Excluded from CUDA tests'
test_ConvTranspose2d_output_size (__main__.TestNN) ... ok
test_ConvTranspose2d_output_size_downsample_upsample (__main__.TestNN) ... ok
test_ConvTranspose2d_with_long_tensor (__main__.TestNN) ... ok
test_ConvTranspose2d_with_long_tensor_cuda (__main__.TestNN) ... skipped 'Excluded from CUDA tests'
test_ConvTranspose3d (__main__.TestNN) ... ok
test_ConvTranspose3d_correct_output_size (__main__.TestNN) ... ok
test_ConvTranspose3d_cuda (__main__.TestNN) ... ok
test_ConvTranspose3d_dilated (__main__.TestNN) ... ok
test_ConvTranspose3d_dilated_cuda (__main__.TestNN) ... ok
test_CosineEmbeddingLoss (__main__.TestNN) ... ok
test_CosineEmbeddingLoss_cuda_double (__main__.TestNN) ... ok
test_CosineEmbeddingLoss_cuda_float (__main__.TestNN) ... ok
test_CosineEmbeddingLoss_cuda_half (__main__.TestNN) ... ok
test_CosineEmbeddingLoss_margin (__main__.TestNN) ... ok
test_CosineEmbeddingLoss_margin_cuda_double (__main__.TestNN) ... ok
test_CosineEmbeddingLoss_margin_cuda_float (__main__.TestNN) ... ok
test_CosineEmbeddingLoss_margin_cuda_half (__main__.TestNN) ... ok
test_CosineEmbeddingLoss_margin_sum_reduction (__main__.TestNN) ... ok
test_CosineEmbeddingLoss_margin_sum_reduction_cuda_double (__main__.TestNN) ... ok
test_CosineEmbeddingLoss_margin_sum_reduction_cuda_float (__main__.TestNN) ... ok
test_CosineEmbeddingLoss_margin_sum_reduction_cuda_half (__main__.TestNN) ... ok
test_CosineEmbeddingLoss_sum_reduction (__main__.TestNN) ... ok
test_CosineEmbeddingLoss_sum_reduction_cuda_double (__main__.TestNN) ... ok
test_CosineEmbeddingLoss_sum_reduction_cuda_float (__main__.TestNN) ... ok
test_CosineEmbeddingLoss_sum_reduction_cuda_half (__main__.TestNN) ... ok
test_CrossEntropyLoss (__main__.TestNN) ... ok
test_CrossEntropyLoss_cuda_double (__main__.TestNN) ... ok
test_CrossEntropyLoss_cuda_float (__main__.TestNN) ... ok
test_CrossEntropyLoss_cuda_half (__main__.TestNN) ... ok
test_CrossEntropyLoss_weights (__main__.TestNN) ... ok
test_CrossEntropyLoss_weights_cuda_double (__main__.TestNN) ... ok
test_CrossEntropyLoss_weights_cuda_float (__main__.TestNN) ... ok
test_CrossEntropyLoss_weights_cuda_half (__main__.TestNN) ... ok
test_CrossMapLRN2d (__main__.TestNN) ... ok
test_CrossMapLRN2d_cuda (__main__.TestNN) ... ok
test_ELU (__main__.TestNN) ... ok
test_ELU_cuda (__main__.TestNN) ... ok
test_ELU_scalar (__main__.TestNN) ... ok
test_ELU_scalar_cuda (__main__.TestNN) ... ok
test_Embedding (__main__.TestNN) ... ok
test_EmbeddingBag_alert_nondeterministic_cuda (__main__.TestNN) ... ok
test_EmbeddingBag_max (__main__.TestNN) ... ok
test_EmbeddingBag_max_cuda (__main__.TestNN) ... ok
test_EmbeddingBag_mean (__main__.TestNN) ... ok
test_EmbeddingBag_mean_cuda (__main__.TestNN) ... ok
test_EmbeddingBag_sparse (__main__.TestNN) ... ok
test_EmbeddingBag_sparse_cuda (__main__.TestNN) ... ok
test_EmbeddingBag_sum (__main__.TestNN) ... ok
test_EmbeddingBag_sum_cuda (__main__.TestNN) ... ok
test_Embedding_cuda (__main__.TestNN) ... ok
test_Embedding_sparse (__main__.TestNN) ... ok
test_Embedding_sparse_cuda (__main__.TestNN) ... ok
test_FeatureAlphaDropout (__main__.TestNN) ... ok
test_Flatten (__main__.TestNN) ... ok
test_Flatten_cuda (__main__.TestNN) ... ok
test_Fold (__main__.TestNN) ... ok
test_Fold_cuda (__main__.TestNN) ... ok
test_Fold_int_input (__main__.TestNN) ... ok
test_Fold_int_input_cuda (__main__.TestNN) ... ok
test_FractionalMaxPool2d_alert_nondeterministic_cuda (__main__.TestNN) ... ok
test_FractionalMaxPool2d_ratio (__main__.TestNN) ... ok
test_FractionalMaxPool2d_ratio_cuda (__main__.TestNN) ... ok
test_FractionalMaxPool2d_size (__main__.TestNN) ... ok
test_FractionalMaxPool2d_size_cuda (__main__.TestNN) ... ok
test_FractionalMaxPool3d_alert_nondeterministic_cuda (__main__.TestNN) ... ok
test_FractionalMaxPool3d_asymsize (__main__.TestNN) ... ok
test_FractionalMaxPool3d_asymsize_cuda (__main__.TestNN) ... ok
test_FractionalMaxPool3d_ratio (__main__.TestNN) ... ok
test_FractionalMaxPool3d_ratio_cuda (__main__.TestNN) ... ok
test_FractionalMaxPool3d_size (__main__.TestNN) ... ok
test_FractionalMaxPool3d_size_cuda (__main__.TestNN) ... ok
test_GELU (__main__.TestNN) ... ok
test_GELU_cuda (__main__.TestNN) ... ok
test_GELU_scalar (__main__.TestNN) ... ok
test_GELU_scalar_cuda (__main__.TestNN) ... ok
test_GLU (__main__.TestNN) ... ok
test_GLU_cuda (__main__.TestNN) ... ok
test_GLU_dim (__main__.TestNN) ... ok
test_GLU_dim_cuda (__main__.TestNN) ... ok
test_GroupNorm_1d_affine (__main__.TestNN) ... ok
test_GroupNorm_1d_affine_GN (__main__.TestNN) ... ok
test_GroupNorm_1d_affine_GN_cuda (__main__.TestNN) ... ok
test_GroupNorm_1d_affine_GN_eval (__main__.TestNN) ... ok
test_GroupNorm_1d_affine_GN_eval_cuda (__main__.TestNN) ... ok
test_GroupNorm_1d_affine_cuda (__main__.TestNN) ... ok
test_GroupNorm_1d_affine_eval (__main__.TestNN) ... ok
test_GroupNorm_1d_affine_eval_cuda (__main__.TestNN) ... ok
test_GroupNorm_1d_affine_large_batch_cuda (__main__.TestNN) ... ok
test_GroupNorm_1d_affine_large_batch_eval_cuda (__main__.TestNN) ... ok
test_GroupNorm_1d_no_affine_IN (__main__.TestNN) ... ok
test_GroupNorm_1d_no_affine_IN_cuda (__main__.TestNN) ... ok
test_GroupNorm_1d_no_affine_IN_eval (__main__.TestNN) ... ok
test_GroupNorm_1d_no_affine_IN_eval_cuda (__main__.TestNN) ... ok
test_GroupNorm_1d_no_affine_LN (__main__.TestNN) ... ok
test_GroupNorm_1d_no_affine_LN_cuda (__main__.TestNN) ... ok
test_GroupNorm_1d_no_affine_LN_eval (__main__.TestNN) ... ok
test_GroupNorm_1d_no_affine_LN_eval_cuda (__main__.TestNN) ... ok
test_GroupNorm_2d_affine (__main__.TestNN) ... ok
test_GroupNorm_2d_affine_cuda (__main__.TestNN) ... ok
test_GroupNorm_2d_affine_eval (__main__.TestNN) ... ok
test_GroupNorm_2d_affine_eval_cuda (__main__.TestNN) ... ok
test_GroupNorm_2d_affine_large_feature_cuda (__main__.TestNN) ... ok
test_GroupNorm_2d_affine_large_feature_eval_cuda (__main__.TestNN) ... ok
test_GroupNorm_2d_no_affine_IN (__main__.TestNN) ... ok
test_GroupNorm_2d_no_affine_IN_cuda (__main__.TestNN) ... ok
test_GroupNorm_2d_no_affine_IN_eval (__main__.TestNN) ... ok
test_GroupNorm_2d_no_affine_IN_eval_cuda (__main__.TestNN) ... ok
test_GroupNorm_2d_no_affine_LN (__main__.TestNN) ... ok
test_GroupNorm_2d_no_affine_LN_cuda (__main__.TestNN) ... ok
test_GroupNorm_2d_no_affine_LN_eval (__main__.TestNN) ... ok
test_GroupNorm_2d_no_affine_LN_eval_cuda (__main__.TestNN) ... ok
test_Hardshrink (__main__.TestNN) ... ok
test_Hardshrink_cuda (__main__.TestNN) ... ok
test_Hardshrink_scalar (__main__.TestNN) ... ok
test_Hardshrink_scalar_cuda (__main__.TestNN) ... ok
test_Hardtanh (__main__.TestNN) ... ok
test_Hardtanh_cuda (__main__.TestNN) ... ok
test_Hardtanh_scalar (__main__.TestNN) ... ok
test_Hardtanh_scalar_cuda (__main__.TestNN) ... ok
test_HingeEmbeddingLoss (__main__.TestNN) ... ok
test_HingeEmbeddingLoss_cuda_double (__main__.TestNN) ... ok
test_HingeEmbeddingLoss_cuda_float (__main__.TestNN) ... ok
test_HingeEmbeddingLoss_cuda_half (__main__.TestNN) ... ok
test_HingeEmbeddingLoss_margin (__main__.TestNN) ... ok
test_HingeEmbeddingLoss_margin_cuda_double (__main__.TestNN) ... ok
test_HingeEmbeddingLoss_margin_cuda_float (__main__.TestNN) ... ok
test_HingeEmbeddingLoss_margin_cuda_half (__main__.TestNN) ... ok
test_HingeEmbeddingLoss_margin_no_reduce (__main__.TestNN) ... ok
test_HingeEmbeddingLoss_margin_no_reduce_cuda (__main__.TestNN) ... ok
test_HingeEmbeddingLoss_margin_sum_reduction (__main__.TestNN) ... ok
test_HingeEmbeddingLoss_margin_sum_reduction_cuda_double (__main__.TestNN) ... ok
test_HingeEmbeddingLoss_margin_sum_reduction_cuda_float (__main__.TestNN) ... ok
test_HingeEmbeddingLoss_margin_sum_reduction_cuda_half (__main__.TestNN) ... ok
test_HingeEmbeddingLoss_no_reduce (__main__.TestNN) ... ok
test_HingeEmbeddingLoss_no_reduce_cuda (__main__.TestNN) ... ok
test_HingeEmbeddingLoss_scalar_margin (__main__.TestNN) ... ok
test_HingeEmbeddingLoss_scalar_margin_cuda_double (__main__.TestNN) ... ok
test_HingeEmbeddingLoss_scalar_margin_cuda_float (__main__.TestNN) ... ok
test_HingeEmbeddingLoss_scalar_margin_cuda_half (__main__.TestNN) ... ok
test_HingeEmbeddingLoss_scalar_margin_sum_reduction (__main__.TestNN) ... ok
test_HingeEmbeddingLoss_scalar_margin_sum_reduction_cuda_double (__main__.TestNN) ... ok
test_HingeEmbeddingLoss_scalar_margin_sum_reduction_cuda_float (__main__.TestNN) ... ok
test_HingeEmbeddingLoss_scalar_margin_sum_reduction_cuda_half (__main__.TestNN) ... ok
test_HingeEmbeddingLoss_sum_reduction (__main__.TestNN) ... ok
test_HingeEmbeddingLoss_sum_reduction_cuda_double (__main__.TestNN) ... ok
test_HingeEmbeddingLoss_sum_reduction_cuda_float (__main__.TestNN) ... ok
test_HingeEmbeddingLoss_sum_reduction_cuda_half (__main__.TestNN) ... ok
test_HuberLoss (__main__.TestNN) ... ok
test_HuberLoss_cuda_bfloat16 (__main__.TestNN) ... ok
test_HuberLoss_cuda_double (__main__.TestNN) ... ok
test_HuberLoss_cuda_float (__main__.TestNN) ... ok
test_HuberLoss_cuda_half (__main__.TestNN) ... ok
test_HuberLoss_delta (__main__.TestNN) ... ok
test_HuberLoss_delta_cuda (__main__.TestNN) ... ok
test_HuberLoss_sum_reduction (__main__.TestNN) ... ok
test_HuberLoss_sum_reduction_cuda_bfloat16 (__main__.TestNN) ... ok
test_HuberLoss_sum_reduction_cuda_double (__main__.TestNN) ... ok
test_HuberLoss_sum_reduction_cuda_float (__main__.TestNN) ... ok
test_HuberLoss_sum_reduction_cuda_half (__main__.TestNN) ... ok
test_InstanceNorm1d (__main__.TestNN) ... ok
test_InstanceNorm1d_cuda (__main__.TestNN) ... ok
test_InstanceNorm1d_eval (__main__.TestNN) ... ok
test_InstanceNorm1d_eval_cuda (__main__.TestNN) ... ok
test_InstanceNorm1d_tracking_stats (__main__.TestNN) ... ok
test_InstanceNorm1d_tracking_stats_cuda (__main__.TestNN) ... ok
test_InstanceNorm1d_tracking_stats_eval (__main__.TestNN) ... ok
test_InstanceNorm1d_tracking_stats_eval_cuda (__main__.TestNN) ... ok
test_InstanceNorm2d (__main__.TestNN) ... ok
test_InstanceNorm2d_cuda (__main__.TestNN) ... ok
test_InstanceNorm2d_eval (__main__.TestNN) ... ok
test_InstanceNorm2d_eval_cuda (__main__.TestNN) ... ok
test_InstanceNorm2d_tracking_stats (__main__.TestNN) ... ok
test_InstanceNorm2d_tracking_stats_cuda (__main__.TestNN) ... ok
test_InstanceNorm2d_tracking_stats_eval (__main__.TestNN) ... ok
test_InstanceNorm2d_tracking_stats_eval_cuda (__main__.TestNN) ... ok
test_InstanceNorm3d (__main__.TestNN) ... ok
test_InstanceNorm3d_cuda (__main__.TestNN) ... ok
test_InstanceNorm3d_eval (__main__.TestNN) ... ok
test_InstanceNorm3d_eval_cuda (__main__.TestNN) ... ok
test_InstanceNorm3d_tracking_stats (__main__.TestNN) ... ok
test_InstanceNorm3d_tracking_stats_cuda (__main__.TestNN) ... ok
test_InstanceNorm3d_tracking_stats_eval (__main__.TestNN) ... ok
test_InstanceNorm3d_tracking_stats_eval_cuda (__main__.TestNN) ... ok
test_KLDivLoss (__main__.TestNN) ... /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
ok
test_KLDivLoss_batch_mean (__main__.TestNN) ... ok
test_KLDivLoss_batch_mean_log_target (__main__.TestNN) ... ok
test_KLDivLoss_cuda_double (__main__.TestNN) ... ok
test_KLDivLoss_cuda_float (__main__.TestNN) ... ok
test_KLDivLoss_cuda_half (__main__.TestNN) ... ok
test_KLDivLoss_log_target (__main__.TestNN) ... /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
ok
test_KLDivLoss_log_target_cuda_double (__main__.TestNN) ... ok
test_KLDivLoss_log_target_cuda_float (__main__.TestNN) ... ok
test_KLDivLoss_log_target_cuda_half (__main__.TestNN) ... ok
test_KLDivLoss_log_target_sum_reduction (__main__.TestNN) ... ok
test_KLDivLoss_log_target_sum_reduction_cuda_double (__main__.TestNN) ... ok
test_KLDivLoss_log_target_sum_reduction_cuda_float (__main__.TestNN) ... ok
test_KLDivLoss_log_target_sum_reduction_cuda_half (__main__.TestNN) ... ok
test_KLDivLoss_no_reduce (__main__.TestNN) ... ok
test_KLDivLoss_no_reduce_cuda (__main__.TestNN) ... ok
test_KLDivLoss_no_reduce_log_target (__main__.TestNN) ... ok
test_KLDivLoss_no_reduce_log_target_cuda (__main__.TestNN) ... ok
test_KLDivLoss_no_reduce_scalar (__main__.TestNN) ... ok
test_KLDivLoss_no_reduce_scalar_cuda (__main__.TestNN) ... ok
test_KLDivLoss_no_reduce_scalar_log_target (__main__.TestNN) ... ok
test_KLDivLoss_no_reduce_scalar_log_target_cuda (__main__.TestNN) ... ok
test_KLDivLoss_scalar (__main__.TestNN) ... /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
ok
test_KLDivLoss_scalar_cuda_double (__main__.TestNN) ... ok
test_KLDivLoss_scalar_cuda_float (__main__.TestNN) ... ok
test_KLDivLoss_scalar_cuda_half (__main__.TestNN) ... ok
test_KLDivLoss_scalar_log_target (__main__.TestNN) ... /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
ok
test_KLDivLoss_scalar_log_target_cuda_double (__main__.TestNN) ... ok
test_KLDivLoss_scalar_log_target_cuda_float (__main__.TestNN) ... ok
test_KLDivLoss_scalar_log_target_cuda_half (__main__.TestNN) ... ok
test_KLDivLoss_scalar_log_target_sum_reduction (__main__.TestNN) ... ok
test_KLDivLoss_scalar_log_target_sum_reduction_cuda_double (__main__.TestNN) ... ok
test_KLDivLoss_scalar_log_target_sum_reduction_cuda_float (__main__.TestNN) ... ok
test_KLDivLoss_scalar_log_target_sum_reduction_cuda_half (__main__.TestNN) ... ok
test_KLDivLoss_scalar_sum_reduction (__main__.TestNN) ... ok
test_KLDivLoss_scalar_sum_reduction_cuda_double (__main__.TestNN) ... ok
test_KLDivLoss_scalar_sum_reduction_cuda_float (__main__.TestNN) ... ok
test_KLDivLoss_scalar_sum_reduction_cuda_half (__main__.TestNN) ... ok
test_KLDivLoss_sum_reduction (__main__.TestNN) ... ok
test_KLDivLoss_sum_reduction_cuda_double (__main__.TestNN) ... ok
test_KLDivLoss_sum_reduction_cuda_float (__main__.TestNN) ... ok
test_KLDivLoss_sum_reduction_cuda_half (__main__.TestNN) ... ok
test_KLDivLoss_with_log_target_no_reduce (__main__.TestNN) ... ok
test_KLDivLoss_with_log_target_no_reduce_cuda (__main__.TestNN) ... ok
test_KLDivLoss_with_target_no_reduce (__main__.TestNN) ... ok
test_KLDivLoss_with_target_no_reduce_cuda (__main__.TestNN) ... ok
test_L1Loss (__main__.TestNN) ... ok
test_L1Loss_cuda_cdouble (__main__.TestNN) ... ok
test_L1Loss_cuda_cfloat (__main__.TestNN) ... ok
test_L1Loss_cuda_double (__main__.TestNN) ... ok
test_L1Loss_cuda_float (__main__.TestNN) ... ok
test_L1Loss_cuda_half (__main__.TestNN) ... ok
test_L1Loss_no_reduce (__main__.TestNN) ... ok
test_L1Loss_no_reduce_complex (__main__.TestNN) ... ok
test_L1Loss_no_reduce_complex_cuda (__main__.TestNN) ... ok
test_L1Loss_no_reduce_cuda (__main__.TestNN) ... ok
test_L1Loss_no_reduce_scalar (__main__.TestNN) ... ok
test_L1Loss_no_reduce_scalar_cuda (__main__.TestNN) ... ok
test_L1Loss_scalar (__main__.TestNN) ... ok
test_L1Loss_scalar_cuda_cdouble (__main__.TestNN) ... ok
test_L1Loss_scalar_cuda_cfloat (__main__.TestNN) ... ok
test_L1Loss_scalar_cuda_double (__main__.TestNN) ... ok
test_L1Loss_scalar_cuda_float (__main__.TestNN) ... ok
test_L1Loss_scalar_cuda_half (__main__.TestNN) ... ok
test_LPPool1d (__main__.TestNN) ... ok
test_LPPool1d_cuda (__main__.TestNN) ... ok
test_LPPool1d_norm (__main__.TestNN) ... ok
test_LPPool1d_norm_cuda (__main__.TestNN) ... ok
test_LPPool2d (__main__.TestNN) ... ok
test_LPPool2d_cuda (__main__.TestNN) ... ok
test_LPPool2d_norm (__main__.TestNN) ... ok
test_LPPool2d_norm_cuda (__main__.TestNN) ... ok
test_LSTM_cell (__main__.TestNN) ... ok
test_LSTM_cell_forward_hidden_size (__main__.TestNN) ... ok
test_LSTM_cell_forward_input_size (__main__.TestNN) ... ok
test_LayerNorm_1d_elementwise_affine (__main__.TestNN) ... ok
test_LayerNorm_1d_elementwise_affine_cuda (__main__.TestNN) ... ok
test_LayerNorm_1d_elementwise_affine_eval (__main__.TestNN) ... ok
test_LayerNorm_1d_elementwise_affine_eval_cuda (__main__.TestNN) ... ok
test_LayerNorm_1d_empty_elementwise_affine (__main__.TestNN) ... ok
test_LayerNorm_1d_empty_elementwise_affine_cuda (__main__.TestNN) ... ok
test_LayerNorm_1d_empty_elementwise_affine_eval (__main__.TestNN) ... ok
test_LayerNorm_1d_empty_elementwise_affine_eval_cuda (__main__.TestNN) ... ok
test_LayerNorm_1d_no_elementwise_affine (__main__.TestNN) ... ok
test_LayerNorm_1d_no_elementwise_affine_cuda (__main__.TestNN) ... ok
test_LayerNorm_1d_no_elementwise_affine_eval (__main__.TestNN) ... ok
test_LayerNorm_1d_no_elementwise_affine_eval_cuda (__main__.TestNN) ... ok
test_LayerNorm_3d_elementwise_affine (__main__.TestNN) ... ok
test_LayerNorm_3d_elementwise_affine_cuda (__main__.TestNN) ... ok
test_LayerNorm_3d_elementwise_affine_eval (__main__.TestNN) ... ok
test_LayerNorm_3d_elementwise_affine_eval_cuda (__main__.TestNN) ... ok
test_LayerNorm_3d_no_elementwise_affine (__main__.TestNN) ... ok
test_LayerNorm_3d_no_elementwise_affine_cuda (__main__.TestNN) ... ok
test_LayerNorm_3d_no_elementwise_affine_eval (__main__.TestNN) ... ok
test_LayerNorm_3d_no_elementwise_affine_eval_cuda (__main__.TestNN) ... ok
test_LeakyReLU (__main__.TestNN) ... ok
test_LeakyReLU_cuda (__main__.TestNN) ... ok
test_LeakyReLU_with_negval (__main__.TestNN) ... ok
test_LeakyReLU_with_negval_cuda (__main__.TestNN) ... ok
test_LeakyReLU_with_negval_scalar (__main__.TestNN) ... ok
test_LeakyReLU_with_negval_scalar_cuda (__main__.TestNN) ... ok
test_LeakyReLU_with_zero_negval (__main__.TestNN) ... ok
test_LeakyReLU_with_zero_negval_cuda (__main__.TestNN) ... ok
test_Linear (__main__.TestNN) ... ok
test_Linear_cuda (__main__.TestNN) ... ok
test_Linear_no_bias (__main__.TestNN) ... ok
test_Linear_no_bias_cuda (__main__.TestNN) ... ok
test_LocalResponseNorm_1d (__main__.TestNN) ... ok
test_LocalResponseNorm_1d_cuda (__main__.TestNN) ... ok
test_LocalResponseNorm_2d_uneven_pad (__main__.TestNN) ... ok
test_LocalResponseNorm_2d_uneven_pad_cuda (__main__.TestNN) ... ok
test_LocalResponseNorm_3d_custom_params (__main__.TestNN) ... ok
test_LocalResponseNorm_3d_custom_params_cuda (__main__.TestNN) ... ok
test_LogSigmoid (__main__.TestNN) ... ok
test_LogSigmoid_cuda (__main__.TestNN) ... ok
test_LogSigmoid_scalar (__main__.TestNN) ... ok
test_LogSigmoid_scalar_cuda (__main__.TestNN) ... ok
test_LogSoftmax (__main__.TestNN) ... ok
test_LogSoftmax_cuda (__main__.TestNN) ... ok
test_LogSoftmax_multiparam (__main__.TestNN) ... ok
test_LogSoftmax_multiparam_cuda (__main__.TestNN) ... ok
test_LogSoftmax_multiparam_scalar (__main__.TestNN) ... ok
test_LogSoftmax_multiparam_scalar_cuda (__main__.TestNN) ... ok
test_MSELoss (__main__.TestNN) ... ok
test_MSELoss_cuda_double (__main__.TestNN) ... ok
test_MSELoss_cuda_float (__main__.TestNN) ... ok
test_MSELoss_cuda_half (__main__.TestNN) ... ok
test_MSELoss_no_reduce (__main__.TestNN) ... ok
test_MSELoss_no_reduce_cuda (__main__.TestNN) ... ok
test_MSELoss_no_reduce_scalar (__main__.TestNN) ... ok
test_MSELoss_no_reduce_scalar_cuda (__main__.TestNN) ... ok
test_MSELoss_prec (__main__.TestNN) ... ok
test_MSELoss_prec_cuda_bfloat16 (__main__.TestNN) ... ok
test_MSELoss_prec_cuda_double (__main__.TestNN) ... ok
test_MSELoss_prec_cuda_float (__main__.TestNN) ... ok
test_MSELoss_prec_cuda_half (__main__.TestNN) ... ok
test_MSELoss_scalar (__main__.TestNN) ... ok
test_MSELoss_scalar_cuda_bfloat16 (__main__.TestNN) ... ok
test_MSELoss_scalar_cuda_double (__main__.TestNN) ... ok
test_MSELoss_scalar_cuda_float (__main__.TestNN) ... ok
test_MSELoss_scalar_cuda_half (__main__.TestNN) ... ok
test_MSELoss_scalar_sum_reduction (__main__.TestNN) ... ok
test_MSELoss_scalar_sum_reduction_cuda_bfloat16 (__main__.TestNN) ... ok
test_MSELoss_scalar_sum_reduction_cuda_double (__main__.TestNN) ... ok
test_MSELoss_scalar_sum_reduction_cuda_float (__main__.TestNN) ... ok
test_MSELoss_scalar_sum_reduction_cuda_half (__main__.TestNN) ... ok
test_MSELoss_sum_reduction (__main__.TestNN) ... ok
test_MSELoss_sum_reduction_cuda_double (__main__.TestNN) ... ok
test_MSELoss_sum_reduction_cuda_float (__main__.TestNN) ... ok
test_MSELoss_sum_reduction_cuda_half (__main__.TestNN) ... ok
test_MarginRankingLoss (__main__.TestNN) ... ok
test_MarginRankingLoss_cuda_double (__main__.TestNN) ... ok
test_MarginRankingLoss_cuda_float (__main__.TestNN) ... ok
test_MarginRankingLoss_cuda_half (__main__.TestNN) ... ok
test_MarginRankingLoss_margin (__main__.TestNN) ... ok
test_MarginRankingLoss_margin_cuda_double (__main__.TestNN) ... ok
test_MarginRankingLoss_margin_cuda_float (__main__.TestNN) ... ok
test_MarginRankingLoss_margin_cuda_half (__main__.TestNN) ... ok
test_MarginRankingLoss_margin_sum_reduction (__main__.TestNN) ... ok
test_MarginRankingLoss_margin_sum_reduction_cuda_double (__main__.TestNN) ... ok
test_MarginRankingLoss_margin_sum_reduction_cuda_float (__main__.TestNN) ... ok
test_MarginRankingLoss_margin_sum_reduction_cuda_half (__main__.TestNN) ... ok
test_MarginRankingLoss_sum_reduction (__main__.TestNN) ... ok
test_MarginRankingLoss_sum_reduction_cuda_double (__main__.TestNN) ... ok
test_MarginRankingLoss_sum_reduction_cuda_float (__main__.TestNN) ... ok
test_MarginRankingLoss_sum_reduction_cuda_half (__main__.TestNN) ... ok
test_MaxPool1d (__main__.TestNN) ... ok
test_MaxPool1d_cuda (__main__.TestNN) ... ok
test_MaxPool1d_stride (__main__.TestNN) ... ok
test_MaxPool1d_stride_cuda (__main__.TestNN) ... ok
test_MaxPool2d_3d_input (__main__.TestNN) ... ok
test_MaxPool2d_3d_input_cuda (__main__.TestNN) ... ok
test_MaxPool2d_4d_input (__main__.TestNN) ... ok
test_MaxPool2d_4d_input_cuda (__main__.TestNN) ... ok
test_MaxPool3d (__main__.TestNN) ... ok
test_MaxPool3d_alert_nondeterministic_cuda (__main__.TestNN) ... ok
test_MaxPool3d_cuda (__main__.TestNN) ... ok
test_MaxPool3d_stride (__main__.TestNN) ... ok
test_MaxPool3d_stride_cuda (__main__.TestNN) ... ok
test_MaxPool3d_stride_padding (__main__.TestNN) ... ok
test_MaxPool3d_stride_padding_cuda (__main__.TestNN) ... ok
test_MaxUnpool1d_net (__main__.TestNN) ... ok
test_MaxUnpool1d_net_cuda (__main__.TestNN) ... ok
test_MaxUnpool2d_net (__main__.TestNN) ... ok
test_MaxUnpool2d_net_cuda (__main__.TestNN) ... ok
test_MaxUnpool2d_output_size (__main__.TestNN) ... ok
test_MaxUnpool3d_net (__main__.TestNN) ... ok
test_MaxUnpool3d_net_cuda (__main__.TestNN) ... ok
test_ModuleDict (__main__.TestNN) ... ok
test_ModuleList (__main__.TestNN) ... ok
test_MultiLabelMarginLoss (__main__.TestNN) ... ok
test_MultiLabelMarginLoss_0d_no_reduce (__main__.TestNN) ... ok
test_MultiLabelMarginLoss_0d_no_reduce_cuda (__main__.TestNN) ... ok
test_MultiLabelMarginLoss_1d (__main__.TestNN) ... ok
test_MultiLabelMarginLoss_1d_cuda_bfloat16 (__main__.TestNN) ... ok
test_MultiLabelMarginLoss_1d_cuda_double (__main__.TestNN) ... ok
test_MultiLabelMarginLoss_1d_cuda_float (__main__.TestNN) ... ok
test_MultiLabelMarginLoss_1d_cuda_half (__main__.TestNN) ... ok
test_MultiLabelMarginLoss_1d_no_reduce (__main__.TestNN) ... ok
test_MultiLabelMarginLoss_1d_no_reduce_cuda (__main__.TestNN) ... ok
test_MultiLabelMarginLoss_1d_sum_reduction (__main__.TestNN) ... ok
test_MultiLabelMarginLoss_1d_sum_reduction_cuda_bfloat16 (__main__.TestNN) ... ok
test_MultiLabelMarginLoss_1d_sum_reduction_cuda_double (__main__.TestNN) ... ok
test_MultiLabelMarginLoss_1d_sum_reduction_cuda_float (__main__.TestNN) ... ok
test_MultiLabelMarginLoss_1d_sum_reduction_cuda_half (__main__.TestNN) ... ok
test_MultiLabelMarginLoss_cuda_bfloat16 (__main__.TestNN) ... ok
test_MultiLabelMarginLoss_cuda_double (__main__.TestNN) ... ok
test_MultiLabelMarginLoss_cuda_float (__main__.TestNN) ... ok
test_MultiLabelMarginLoss_cuda_half (__main__.TestNN) ... ok
test_MultiLabelMarginLoss_index_neg (__main__.TestNN) ... ok
test_MultiLabelMarginLoss_index_neg_cuda (__main__.TestNN) ... ok
test_MultiLabelMarginLoss_no_reduce (__main__.TestNN) ... ok
test_MultiLabelMarginLoss_no_reduce_cuda (__main__.TestNN) ... ok
test_MultiLabelMarginLoss_sum_reduction (__main__.TestNN) ... ok
test_MultiLabelMarginLoss_sum_reduction_cuda_bfloat16 (__main__.TestNN) ... ok
test_MultiLabelMarginLoss_sum_reduction_cuda_double (__main__.TestNN) ... ok
test_MultiLabelMarginLoss_sum_reduction_cuda_float (__main__.TestNN) ... ok
test_MultiLabelMarginLoss_sum_reduction_cuda_half (__main__.TestNN) ... ok
test_MultiLabelSoftMarginLoss (__main__.TestNN) ... ok
test_MultiLabelSoftMarginLoss_cuda_double (__main__.TestNN) ... ok
test_MultiLabelSoftMarginLoss_cuda_float (__main__.TestNN) ... ok
test_MultiLabelSoftMarginLoss_cuda_half (__main__.TestNN) ... ok
test_MultiLabelSoftMarginLoss_no_reduce (__main__.TestNN) ... ok
test_MultiLabelSoftMarginLoss_no_reduce_cuda (__main__.TestNN) ... ok
test_MultiLabelSoftMarginLoss_weights (__main__.TestNN) ... ok
test_MultiLabelSoftMarginLoss_weights_cuda_double (__main__.TestNN) ... ok
test_MultiLabelSoftMarginLoss_weights_cuda_float (__main__.TestNN) ... ok
test_MultiLabelSoftMarginLoss_weights_cuda_half (__main__.TestNN) ... ok
test_MultiLabelSoftMarginLoss_weights_no_reduce (__main__.TestNN) ... ok
test_MultiLabelSoftMarginLoss_weights_no_reduce_cuda (__main__.TestNN) ... ok
test_MultiLabelSoftMarginLoss_weights_sum_reduction (__main__.TestNN) ... ok
test_MultiLabelSoftMarginLoss_weights_sum_reduction_cuda_double (__main__.TestNN) ... ok
test_MultiLabelSoftMarginLoss_weights_sum_reduction_cuda_float (__main__.TestNN) ... ok
test_MultiLabelSoftMarginLoss_weights_sum_reduction_cuda_half (__main__.TestNN) ... ok
test_MultiMarginLoss (__main__.TestNN) ... ok
test_MultiMarginLoss_1d (__main__.TestNN) ... ok
test_MultiMarginLoss_1d_cuda_double (__main__.TestNN) ... ok
test_MultiMarginLoss_1d_cuda_float (__main__.TestNN) ... ok
test_MultiMarginLoss_1d_cuda_half (__main__.TestNN) ... ok
test_MultiMarginLoss_1d_no_reduce (__main__.TestNN) ... ok
test_MultiMarginLoss_1d_no_reduce_cuda (__main__.TestNN) ... ok
test_MultiMarginLoss_1d_sum_reduction (__main__.TestNN) ... ok
test_MultiMarginLoss_1d_sum_reduction_cuda_double (__main__.TestNN) ... ok
test_MultiMarginLoss_1d_sum_reduction_cuda_float (__main__.TestNN) ... ok
test_MultiMarginLoss_1d_sum_reduction_cuda_half (__main__.TestNN) ... ok
test_MultiMarginLoss_cuda_double (__main__.TestNN) ... ok
test_MultiMarginLoss_cuda_float (__main__.TestNN) ... ok
test_MultiMarginLoss_cuda_half (__main__.TestNN) ... ok
test_MultiMarginLoss_margin (__main__.TestNN) ... ok
test_MultiMarginLoss_margin_cuda_double (__main__.TestNN) ... ok
test_MultiMarginLoss_margin_cuda_float (__main__.TestNN) ... ok
test_MultiMarginLoss_margin_cuda_half (__main__.TestNN) ... ok
test_MultiMarginLoss_margin_no_reduce (__main__.TestNN) ... ok
test_MultiMarginLoss_margin_no_reduce_cuda (__main__.TestNN) ... ok
test_MultiMarginLoss_margin_sum_reduction (__main__.TestNN) ... ok
test_MultiMarginLoss_margin_sum_reduction_cuda_double (__main__.TestNN) ... ok
test_MultiMarginLoss_margin_sum_reduction_cuda_float (__main__.TestNN) ... ok
test_MultiMarginLoss_margin_sum_reduction_cuda_half (__main__.TestNN) ... ok
test_MultiMarginLoss_no_reduce (__main__.TestNN) ... ok
test_MultiMarginLoss_no_reduce_cuda (__main__.TestNN) ... ok
test_MultiMarginLoss_p (__main__.TestNN) ... ok
test_MultiMarginLoss_p_cuda_double (__main__.TestNN) ... ok
test_MultiMarginLoss_p_cuda_float (__main__.TestNN) ... ok
test_MultiMarginLoss_p_cuda_half (__main__.TestNN) ... ok
test_MultiMarginLoss_p_no_reduce (__main__.TestNN) ... ok
test_MultiMarginLoss_p_no_reduce_cuda (__main__.TestNN) ... ok
test_MultiMarginLoss_p_sum_reduction (__main__.TestNN) ... ok
test_MultiMarginLoss_p_sum_reduction_cuda_double (__main__.TestNN) ... ok
test_MultiMarginLoss_p_sum_reduction_cuda_float (__main__.TestNN) ... ok
test_MultiMarginLoss_p_sum_reduction_cuda_half (__main__.TestNN) ... ok
test_MultiMarginLoss_sum_reduction (__main__.TestNN) ... ok
test_MultiMarginLoss_sum_reduction_cuda_double (__main__.TestNN) ... ok
test_MultiMarginLoss_sum_reduction_cuda_float (__main__.TestNN) ... ok
test_MultiMarginLoss_sum_reduction_cuda_half (__main__.TestNN) ... ok
test_MultiMarginLoss_weights (__main__.TestNN) ... ok
test_MultiMarginLoss_weights_cuda_double (__main__.TestNN) ... ok
test_MultiMarginLoss_weights_cuda_float (__main__.TestNN) ... ok
test_MultiMarginLoss_weights_cuda_half (__main__.TestNN) ... ok
test_MultiMarginLoss_weights_no_reduce (__main__.TestNN) ... ok
test_MultiMarginLoss_weights_no_reduce_cuda (__main__.TestNN) ... ok
test_MultiMarginLoss_weights_sum_reduction (__main__.TestNN) ... ok
test_MultiMarginLoss_weights_sum_reduction_cuda_double (__main__.TestNN) ... ok
test_MultiMarginLoss_weights_sum_reduction_cuda_float (__main__.TestNN) ... ok
test_MultiMarginLoss_weights_sum_reduction_cuda_half (__main__.TestNN) ... ok
test_NLLLoss (__main__.TestNN) ... ok
test_NLLLoss2d_no_reduce (__main__.TestNN) ... ok
test_NLLLoss2d_no_reduce_cuda (__main__.TestNN) ... ok
test_NLLLoss2d_no_reduce_ignore_index (__main__.TestNN) ... ok
test_NLLLoss2d_no_reduce_ignore_index_cuda (__main__.TestNN) ... ok
test_NLLLoss2d_no_reduce_weights (__main__.TestNN) ... ok
test_NLLLoss2d_no_reduce_weights_cuda (__main__.TestNN) ... ok
test_NLLLossNd_no_reduce (__main__.TestNN) ... ok
test_NLLLossNd_no_reduce_cuda (__main__.TestNN) ... ok
test_NLLLossNd_no_reduce_ignore_index (__main__.TestNN) ... ok
test_NLLLossNd_no_reduce_ignore_index_cuda (__main__.TestNN) ... ok
test_NLLLossNd_no_reduce_weights (__main__.TestNN) ... ok
test_NLLLossNd_no_reduce_weights_cuda (__main__.TestNN) ... ok
test_NLLLoss_2d (__main__.TestNN) ... ok
test_NLLLoss_2d_alert_nondeterministic_cuda_double (__main__.TestNN) ... ok
test_NLLLoss_2d_alert_nondeterministic_cuda_float (__main__.TestNN) ... ok
test_NLLLoss_2d_alert_nondeterministic_cuda_half (__main__.TestNN) ... ok
test_NLLLoss_2d_alert_nondeterministic_sum_reduction_cuda_double (__main__.TestNN) ... ok
test_NLLLoss_2d_alert_nondeterministic_sum_reduction_cuda_float (__main__.TestNN) ... ok
test_NLLLoss_2d_alert_nondeterministic_sum_reduction_cuda_half (__main__.TestNN) ... ok
test_NLLLoss_2d_cuda_bfloat16 (__main__.TestNN) ... ok
test_NLLLoss_2d_cuda_double (__main__.TestNN) ... ok
test_NLLLoss_2d_cuda_float (__main__.TestNN) ... ok
test_NLLLoss_2d_cuda_half (__main__.TestNN) ... ok
test_NLLLoss_2d_ignore_index (__main__.TestNN) ... ok
test_NLLLoss_2d_ignore_index_cuda_bfloat16 (__main__.TestNN) ... ok
test_NLLLoss_2d_ignore_index_cuda_double (__main__.TestNN) ... ok
test_NLLLoss_2d_ignore_index_cuda_float (__main__.TestNN) ... ok
test_NLLLoss_2d_ignore_index_cuda_half (__main__.TestNN) ... ok
test_NLLLoss_2d_sum_reduction (__main__.TestNN) ... ok
test_NLLLoss_2d_sum_reduction_cuda_bfloat16 (__main__.TestNN) ... ok
test_NLLLoss_2d_sum_reduction_cuda_double (__main__.TestNN) ... ok
test_NLLLoss_2d_sum_reduction_cuda_float (__main__.TestNN) ... ok
test_NLLLoss_2d_sum_reduction_cuda_half (__main__.TestNN) ... ok
test_NLLLoss_2d_weights (__main__.TestNN) ... ok
test_NLLLoss_2d_weights_cuda_bfloat16 (__main__.TestNN) ... ok
test_NLLLoss_2d_weights_cuda_double (__main__.TestNN) ... ok
test_NLLLoss_2d_weights_cuda_float (__main__.TestNN) ... ok
test_NLLLoss_2d_weights_cuda_half (__main__.TestNN) ... ok
test_NLLLoss_cuda_bfloat16 (__main__.TestNN) ... ok
test_NLLLoss_cuda_double (__main__.TestNN) ... ok
test_NLLLoss_cuda_float (__main__.TestNN) ... ok
test_NLLLoss_cuda_half (__main__.TestNN) ... ok
test_NLLLoss_dim_is_3 (__main__.TestNN) ... ok
test_NLLLoss_dim_is_3_cuda_bfloat16 (__main__.TestNN) ... ok
test_NLLLoss_dim_is_3_cuda_double (__main__.TestNN) ... ok
test_NLLLoss_dim_is_3_cuda_float (__main__.TestNN) ... ok
test_NLLLoss_dim_is_3_cuda_half (__main__.TestNN) ... ok
test_NLLLoss_dim_is_3_sum_reduction (__main__.TestNN) ... ok
test_NLLLoss_dim_is_3_sum_reduction_cuda_bfloat16 (__main__.TestNN) ... ok
test_NLLLoss_dim_is_3_sum_reduction_cuda_double (__main__.TestNN) ... ok
test_NLLLoss_dim_is_3_sum_reduction_cuda_float (__main__.TestNN) ... ok
test_NLLLoss_dim_is_3_sum_reduction_cuda_half (__main__.TestNN) ... ok
test_NLLLoss_higher_dim (__main__.TestNN) ... ok
test_NLLLoss_higher_dim_cuda_bfloat16 (__main__.TestNN) ... ok
test_NLLLoss_higher_dim_cuda_double (__main__.TestNN) ... ok
test_NLLLoss_higher_dim_cuda_float (__main__.TestNN) ... ok
test_NLLLoss_higher_dim_cuda_half (__main__.TestNN) ... ok
test_NLLLoss_higher_dim_sum_reduction (__main__.TestNN) ... ok
test_NLLLoss_higher_dim_sum_reduction_cuda_bfloat16 (__main__.TestNN) ... ok
test_NLLLoss_higher_dim_sum_reduction_cuda_double (__main__.TestNN) ... ok
test_NLLLoss_higher_dim_sum_reduction_cuda_float (__main__.TestNN) ... ok
test_NLLLoss_higher_dim_sum_reduction_cuda_half (__main__.TestNN) ... ok
test_NLLLoss_ignore_index (__main__.TestNN) ... ok
test_NLLLoss_ignore_index_cuda_bfloat16 (__main__.TestNN) ... ok
test_NLLLoss_ignore_index_cuda_double (__main__.TestNN) ... ok
test_NLLLoss_ignore_index_cuda_float (__main__.TestNN) ... ok
test_NLLLoss_ignore_index_cuda_half (__main__.TestNN) ... ok
test_NLLLoss_no_reduce (__main__.TestNN) ... ok
test_NLLLoss_no_reduce_cuda (__main__.TestNN) ... ok
test_NLLLoss_no_reduce_ignore_index (__main__.TestNN) ... ok
test_NLLLoss_no_reduce_ignore_index_cuda (__main__.TestNN) ... ok
test_NLLLoss_no_reduce_weights (__main__.TestNN) ... ok
test_NLLLoss_no_reduce_weights_cuda (__main__.TestNN) ... ok
test_NLLLoss_no_reduce_weights_ignore_index (__main__.TestNN) ... ok
test_NLLLoss_no_reduce_weights_ignore_index_cuda (__main__.TestNN) ... ok
test_NLLLoss_no_reduce_weights_ignore_index_neg (__main__.TestNN) ... ok
test_NLLLoss_no_reduce_weights_ignore_index_neg_cuda (__main__.TestNN) ... ok
test_NLLLoss_sum_reduction (__main__.TestNN) ... ok
test_NLLLoss_sum_reduction_cuda_bfloat16 (__main__.TestNN) ... ok
test_NLLLoss_sum_reduction_cuda_double (__main__.TestNN) ... ok
test_NLLLoss_sum_reduction_cuda_float (__main__.TestNN) ... ok
test_NLLLoss_sum_reduction_cuda_half (__main__.TestNN) ... ok
test_NLLLoss_weights (__main__.TestNN) ... ok
test_NLLLoss_weights_cuda_bfloat16 (__main__.TestNN) ... ok
test_NLLLoss_weights_cuda_double (__main__.TestNN) ... ok
test_NLLLoss_weights_cuda_float (__main__.TestNN) ... ok
test_NLLLoss_weights_cuda_half (__main__.TestNN) ... ok
test_NLLLoss_weights_ignore_index (__main__.TestNN) ... ok
test_NLLLoss_weights_ignore_index_cuda_bfloat16 (__main__.TestNN) ... ok
test_NLLLoss_weights_ignore_index_cuda_double (__main__.TestNN) ... ok
test_NLLLoss_weights_ignore_index_cuda_float (__main__.TestNN) ... ok
test_NLLLoss_weights_ignore_index_cuda_half (__main__.TestNN) ... ok
test_NLLLoss_weights_ignore_index_neg (__main__.TestNN) ... ok
test_NLLLoss_weights_ignore_index_neg_cuda_bfloat16 (__main__.TestNN) ... ok
test_NLLLoss_weights_ignore_index_neg_cuda_double (__main__.TestNN) ... ok
test_NLLLoss_weights_ignore_index_neg_cuda_float (__main__.TestNN) ... ok
test_NLLLoss_weights_ignore_index_neg_cuda_half (__main__.TestNN) ... ok
test_PReLU_1d (__main__.TestNN) ... ok
test_PReLU_1d_cuda (__main__.TestNN) ... ok
test_PReLU_1d_multiparam (__main__.TestNN) ... ok
test_PReLU_1d_multiparam_cuda (__main__.TestNN) ... ok
test_PReLU_2d (__main__.TestNN) ... ok
test_PReLU_2d_cuda (__main__.TestNN) ... ok
test_PReLU_2d_multiparam (__main__.TestNN) ... ok
test_PReLU_2d_multiparam_cuda (__main__.TestNN) ... ok
test_PReLU_3d (__main__.TestNN) ... ok
test_PReLU_3d_cuda (__main__.TestNN) ... ok
test_PReLU_3d_multiparam (__main__.TestNN) ... ok
test_PReLU_3d_multiparam_cuda (__main__.TestNN) ... ok
test_PReLU_backward_requires_grad_false (__main__.TestNN) ... ok
test_PReLU_scalar (__main__.TestNN) ... ok
test_PReLU_scalar_cuda (__main__.TestNN) ... ok
test_Padding122112_3dcircular (__main__.TestNN) ... ok
test_Padding122112_3dcircular_cuda (__main__.TestNN) ... ok
test_Padding1221_2dcircular (__main__.TestNN) ... ok
test_Padding1221_2dcircular_cuda (__main__.TestNN) ... ok
test_Padding12_1dcircular (__main__.TestNN) ... ok
test_Padding12_1dcircular_cuda (__main__.TestNN) ... ok
test_Padding2322_2dcircular (__main__.TestNN) ... ok
test_Padding2322_2dcircular_cuda (__main__.TestNN) ... ok
test_Padding31_1dcircular (__main__.TestNN) ... ok
test_Padding31_1dcircular_cuda (__main__.TestNN) ... ok
test_Padding322112_3dcircular (__main__.TestNN) ... ok
test_Padding322112_3dcircular_cuda (__main__.TestNN) ... ok
test_Padding332122_3dcircular (__main__.TestNN) ... ok
test_Padding332122_3dcircular_cuda (__main__.TestNN) ... ok
test_Padding3331_2dcircular (__main__.TestNN) ... ok
test_Padding3331_2dcircular_cuda (__main__.TestNN) ... ok
test_Padding33_1dcircular (__main__.TestNN) ... ok
test_Padding33_1dcircular_cuda (__main__.TestNN) ... ok
test_ParameterDict (__main__.TestNN) ... ok
test_ParameterList (__main__.TestNN) ... ok
test_PixelShuffle (__main__.TestNN) ... ok
test_PixelShuffle_cuda (__main__.TestNN) ... ok
test_PixelUnshuffle (__main__.TestNN) ... ok
test_PixelUnshuffle_cuda (__main__.TestNN) ... ok
test_PoissonNLLLoss_full_loss (__main__.TestNN) ... ok
test_PoissonNLLLoss_full_loss_cuda_double (__main__.TestNN) ... ok
test_PoissonNLLLoss_full_loss_cuda_float (__main__.TestNN) ... ok
test_PoissonNLLLoss_full_loss_cuda_half (__main__.TestNN) ... ok
test_PoissonNLLLoss_full_loss_no_log_input (__main__.TestNN) ... ok
test_PoissonNLLLoss_full_loss_no_log_input_cuda_double (__main__.TestNN) ... ok
test_PoissonNLLLoss_full_loss_no_log_input_cuda_float (__main__.TestNN) ... ok
test_PoissonNLLLoss_full_loss_no_log_input_cuda_half (__main__.TestNN) ... ok
test_PoissonNLLLoss_no_full_loss (__main__.TestNN) ... ok
test_PoissonNLLLoss_no_full_loss_cuda_double (__main__.TestNN) ... ok
test_PoissonNLLLoss_no_full_loss_cuda_float (__main__.TestNN) ... ok
test_PoissonNLLLoss_no_full_loss_cuda_half (__main__.TestNN) ... ok
test_PoissonNLLLoss_no_full_loss_no_log_input (__main__.TestNN) ... ok
test_PoissonNLLLoss_no_full_loss_no_log_input_cuda_double (__main__.TestNN) ... ok
test_PoissonNLLLoss_no_full_loss_no_log_input_cuda_float (__main__.TestNN) ... ok
test_PoissonNLLLoss_no_full_loss_no_log_input_cuda_half (__main__.TestNN) ... ok
test_PoissonNLLLoss_no_reduce (__main__.TestNN) ... ok
test_PoissonNLLLoss_no_reduce_cuda (__main__.TestNN) ... ok
test_RNN_cell (__main__.TestNN) ... ok
test_RNN_cell_forward_hidden_size (__main__.TestNN) ... ok
test_RNN_cell_forward_input_size (__main__.TestNN) ... ok
test_RNN_cell_no_broadcasting (__main__.TestNN) ... ok
test_RNN_change_dropout (__main__.TestNN) ... ok
test_RNN_cpu_vs_cudnn_no_dropout (__main__.TestNN) ... ok
test_RNN_cpu_vs_cudnn_with_dropout (__main__.TestNN) ... ok
test_RNN_cudnn_weight_norm (__main__.TestNN) ... ok
test_RNN_dropout (__main__.TestNN) ... ok
test_RNN_dropout_state (__main__.TestNN) ... ok
test_RNN_nonlinearity (__main__.TestNN) ... ok
test_RReLU (__main__.TestNN) ... ok
test_RReLU_cuda (__main__.TestNN) ... skipped 'Excluded from CUDA tests'
test_RReLU_with_up_down (__main__.TestNN) ... ok
test_RReLU_with_up_down_cuda (__main__.TestNN) ... skipped 'Excluded from CUDA tests'
test_RReLU_with_up_down_scalar (__main__.TestNN) ... ok
test_RReLU_with_up_down_scalar_cuda (__main__.TestNN) ... skipped 'Excluded from CUDA tests'
test_ReLU (__main__.TestNN) ... ok
test_ReLU6 (__main__.TestNN) ... ok
test_ReLU6_cuda (__main__.TestNN) ... ok
test_ReLU6_scalar (__main__.TestNN) ... ok
test_ReLU6_scalar_cuda (__main__.TestNN) ... ok
test_ReLU_cuda (__main__.TestNN) ... ok
test_ReLU_scalar (__main__.TestNN) ... ok
test_ReLU_scalar_cuda (__main__.TestNN) ... ok
test_ReflectionPad1d (__main__.TestNN) ... ok
test_ReflectionPad1d_alert_nondeterministic_cuda (__main__.TestNN) ... ok
test_ReflectionPad1d_complex (__main__.TestNN) ... ok
test_ReflectionPad1d_complex_cuda (__main__.TestNN) ... ok
test_ReflectionPad1d_cuda (__main__.TestNN) ... ok
test_ReflectionPad2d (__main__.TestNN) ... ok
test_ReflectionPad2d_alert_nondeterministic_cuda (__main__.TestNN) ... ok
test_ReflectionPad2d_complex (__main__.TestNN) ... ok
test_ReflectionPad2d_complex_cuda (__main__.TestNN) ... ok
test_ReflectionPad2d_cuda (__main__.TestNN) ... ok
test_ReplicationPad1d (__main__.TestNN) ... ok
test_ReplicationPad1d_alert_nondeterministic_cuda (__main__.TestNN) ... ok
test_ReplicationPad1d_complex (__main__.TestNN) ... ok
test_ReplicationPad1d_complex_cuda (__main__.TestNN) ... ok
test_ReplicationPad1d_cuda (__main__.TestNN) ... ok
test_ReplicationPad2d (__main__.TestNN) ... ok
test_ReplicationPad2d_alert_nondeterministic_cuda (__main__.TestNN) ... ok
test_ReplicationPad2d_complex (__main__.TestNN) ... ok
test_ReplicationPad2d_complex_cuda (__main__.TestNN) ... ok
test_ReplicationPad2d_cuda (__main__.TestNN) ... ok
test_ReplicationPad3d (__main__.TestNN) ... ok
test_ReplicationPad3d_alert_nondeterministic_cuda (__main__.TestNN) ... ok
test_ReplicationPad3d_complex (__main__.TestNN) ... ok
test_ReplicationPad3d_complex_cuda (__main__.TestNN) ... ok
test_ReplicationPad3d_cuda (__main__.TestNN) ... ok
test_SELU (__main__.TestNN) ... ok
test_SELU_cuda (__main__.TestNN) ... ok
test_SELU_scalar (__main__.TestNN) ... ok
test_SELU_scalar_cuda (__main__.TestNN) ... ok
test_Sequential_delitem (__main__.TestNN) ... ok
test_Sequential_getitem (__main__.TestNN) ... ok
test_Sequential_setitem (__main__.TestNN) ... ok
test_Sequential_setitem_named (__main__.TestNN) ... ok
test_SiLU (__main__.TestNN) ... ok
test_SiLU_cuda (__main__.TestNN) ... ok
test_SiLU_scalar (__main__.TestNN) ... ok
test_SiLU_scalar_cuda (__main__.TestNN) ... ok
test_Sigmoid (__main__.TestNN) ... ok
test_Sigmoid_cuda (__main__.TestNN) ... ok
test_Sigmoid_scalar (__main__.TestNN) ... ok
test_Sigmoid_scalar_cuda (__main__.TestNN) ... ok
test_SmoothL1Loss (__main__.TestNN) ... ok
test_SmoothL1Loss_beta (__main__.TestNN) ... ok
test_SmoothL1Loss_beta_cuda (__main__.TestNN) ... ok
test_SmoothL1Loss_cuda_double (__main__.TestNN) ... ok
test_SmoothL1Loss_cuda_float (__main__.TestNN) ... ok
test_SmoothL1Loss_cuda_half (__main__.TestNN) ... ok
test_SmoothL1Loss_no_reduce (__main__.TestNN) ... ok
test_SmoothL1Loss_no_reduce_cuda (__main__.TestNN) ... ok
test_SmoothL1Loss_no_reduce_scalar (__main__.TestNN) ... ok
test_SmoothL1Loss_no_reduce_scalar_cuda (__main__.TestNN) ... ok
test_SmoothL1Loss_scalar (__main__.TestNN) ... ok
test_SmoothL1Loss_scalar_cuda_double (__main__.TestNN) ... ok
test_SmoothL1Loss_scalar_cuda_float (__main__.TestNN) ... ok
test_SmoothL1Loss_scalar_cuda_half (__main__.TestNN) ... ok
test_SmoothL1Loss_scalar_sum_reduction (__main__.TestNN) ... ok
test_SmoothL1Loss_scalar_sum_reduction_cuda_double (__main__.TestNN) ... ok
test_SmoothL1Loss_scalar_sum_reduction_cuda_float (__main__.TestNN) ... ok
test_SmoothL1Loss_scalar_sum_reduction_cuda_half (__main__.TestNN) ... ok
test_SmoothL1Loss_sum_reduction (__main__.TestNN) ... ok
test_SmoothL1Loss_sum_reduction_cuda_double (__main__.TestNN) ... ok
test_SmoothL1Loss_sum_reduction_cuda_float (__main__.TestNN) ... ok
test_SmoothL1Loss_sum_reduction_cuda_half (__main__.TestNN) ... ok
test_SmoothL1Loss_zero_beta (__main__.TestNN) ... ok
test_SmoothL1Loss_zero_beta_cuda (__main__.TestNN) ... ok
test_SoftMarginLoss (__main__.TestNN) ... ok
test_SoftMarginLoss_cuda_double (__main__.TestNN) ... ok
test_SoftMarginLoss_cuda_float (__main__.TestNN) ... ok
test_SoftMarginLoss_cuda_half (__main__.TestNN) ... ok
test_SoftMarginLoss_no_reduce (__main__.TestNN) ... ok
test_SoftMarginLoss_no_reduce_cuda (__main__.TestNN) ... ok
test_SoftMarginLoss_sum_reduction (__main__.TestNN) ... ok
test_SoftMarginLoss_sum_reduction_cuda_double (__main__.TestNN) ... ok
test_SoftMarginLoss_sum_reduction_cuda_float (__main__.TestNN) ... ok
test_SoftMarginLoss_sum_reduction_cuda_half (__main__.TestNN) ... ok
test_Softmax (__main__.TestNN) ... ok
test_Softmax2d (__main__.TestNN) ... ok
test_Softmax2d_cuda (__main__.TestNN) ... ok
test_Softmax_cuda (__main__.TestNN) ... ok
test_Softmax_scalar (__main__.TestNN) ... ok
test_Softmax_scalar_cuda (__main__.TestNN) ... ok
test_Softmin (__main__.TestNN) ... ok
test_Softmin_cuda (__main__.TestNN) ... ok
test_Softmin_multidim (__main__.TestNN) ... ok
test_Softmin_multidim_cuda (__main__.TestNN) ... ok
test_Softmin_scalar (__main__.TestNN) ... ok
test_Softmin_scalar_cuda (__main__.TestNN) ... ok
test_Softplus (__main__.TestNN) ... ok
test_Softplus_beta (__main__.TestNN) ... ok
test_Softplus_beta_cuda (__main__.TestNN) ... ok
test_Softplus_beta_threshold (__main__.TestNN) ... ok
test_Softplus_beta_threshold_cuda (__main__.TestNN) ... ok
test_Softplus_beta_threshold_scalar (__main__.TestNN) ... ok
test_Softplus_beta_threshold_scalar_cuda (__main__.TestNN) ... ok
test_Softplus_cuda (__main__.TestNN) ... ok
test_Softshrink (__main__.TestNN) ... ok
test_Softshrink_cuda (__main__.TestNN) ... ok
test_Softshrink_lambda (__main__.TestNN) ... ok
test_Softshrink_lambda_cuda (__main__.TestNN) ... ok
test_Softshrink_lambda_scalar (__main__.TestNN) ... ok
test_Softshrink_lambda_scalar_cuda (__main__.TestNN) ... ok
test_Softsign (__main__.TestNN) ... ok
test_Softsign_cuda (__main__.TestNN) ... ok
test_Softsign_scalar (__main__.TestNN) ... ok
test_Softsign_scalar_cuda (__main__.TestNN) ... ok
test_Tanh (__main__.TestNN) ... ok
test_Tanh_cuda (__main__.TestNN) ... ok
test_Tanh_scalar (__main__.TestNN) ... ok
test_Tanh_scalar_cuda (__main__.TestNN) ... ok
test_Tanhshrink (__main__.TestNN) ... ok
test_Tanhshrink_cuda (__main__.TestNN) ... ok
test_Tanhshrink_scalar (__main__.TestNN) ... ok
test_Tanhshrink_scalar_cuda (__main__.TestNN) ... ok
test_Threshold_large_value (__main__.TestNN) ... ok
test_Threshold_large_value_cuda (__main__.TestNN) ... ok
test_Threshold_threshold_value (__main__.TestNN) ... ok
test_Threshold_threshold_value_cuda (__main__.TestNN) ... ok
test_Threshold_threshold_value_scalar (__main__.TestNN) ... ok
test_Threshold_threshold_value_scalar_cuda (__main__.TestNN) ... ok
test_TransformerDecoderLayer_gelu_activation (__main__.TestNN) ... ok
test_TransformerDecoderLayer_gelu_activation_cuda (__main__.TestNN) ... ok
test_TransformerDecoderLayer_relu_activation (__main__.TestNN) ... ok
test_TransformerDecoderLayer_relu_activation_cuda (__main__.TestNN) ... ok
test_TransformerEncoderLayer_gelu_activation (__main__.TestNN) ... ok
test_TransformerEncoderLayer_gelu_activation_cuda (__main__.TestNN) ... ok
test_TransformerEncoderLayer_relu_activation (__main__.TestNN) ... ok
test_TransformerEncoderLayer_relu_activation_cuda (__main__.TestNN) ... ok
test_Transformer_cell (__main__.TestNN) ... ok
test_Transformer_multilayer_coder (__main__.TestNN) ... ok
test_Transformer_multilayer_coder_cuda (__main__.TestNN) ... ok
test_Unfold (__main__.TestNN) ... ok
test_Unfold_cuda (__main__.TestNN) ... ok
test_Unfold_int_input (__main__.TestNN) ... ok
test_Unfold_int_input_cuda (__main__.TestNN) ... ok
test_ZeroPad2d (__main__.TestNN) ... ok
test_ZeroPad2d_complex (__main__.TestNN) ... ok
test_ZeroPad2d_complex_cuda (__main__.TestNN) ... ok
test_ZeroPad2d_cuda (__main__.TestNN) ... ok
test_ZeroPad2d_negative_dims (__main__.TestNN) ... ok
test_ZeroPad2d_negative_dims_cuda (__main__.TestNN) ... ok
test_adaptive_log_softmax (__main__.TestNN) ... ok
test_adaptive_pooling_avg_nhwc (__main__.TestNN) ... ok
test_adaptive_pooling_avg_nhwc_launch_config_backward (__main__.TestNN) ... ok
test_adaptive_pooling_avg_nhwc_launch_config_forward (__main__.TestNN) ... ok
test_adaptive_pooling_avg_nhwc_non_contiguous (__main__.TestNN) ... ok
test_adaptive_pooling_input_size (__main__.TestNN) ... ok
test_adaptive_pooling_size_none (__main__.TestNN) ... ok
test_adaptive_pooling_size_overflow (__main__.TestNN) ... ok
test_add_module (__main__.TestNN) ... ok
test_add_module_raises_error_if_attr_exists (__main__.TestNN) ... ok
test_affine_grid (__main__.TestNN) ... ok
test_affine_grid_3d (__main__.TestNN) ... ok
test_affine_grid_error_checking (__main__.TestNN) ... ok
test_assignment (__main__.TestNN) ... ok
test_batchnorm_buffer_update_when_stats_are_not_tracked (__main__.TestNN) ... ok
test_batchnorm_cudnn_half (__main__.TestNN) ... ok
test_batchnorm_cudnn_nhwc (__main__.TestNN) ... skipped "test doesn't currently work on the ROCm stack"
test_batchnorm_large_batch (__main__.TestNN) ... ok
test_batchnorm_nonaffine_cuda_half_input (__main__.TestNN) ... ok
test_batchnorm_raises_error_if_bias_is_not_same_size_as_input (__main__.TestNN) ... ok
test_batchnorm_raises_error_if_less_than_one_value_per_channel (__main__.TestNN) ... ok
test_batchnorm_raises_error_if_running_mean_is_not_same_size_as_input (__main__.TestNN) ... ok
test_batchnorm_raises_error_if_running_var_is_not_same_size_as_input (__main__.TestNN) ... ok
test_batchnorm_raises_error_if_weight_is_not_same_size_as_input (__main__.TestNN) ... ok
test_bce_loss_always_nonnegative (__main__.TestNN) ... ok
test_bce_loss_broadcasts_weights (__main__.TestNN) ... ok
test_bce_loss_input_range (__main__.TestNN) ... ok
test_bce_loss_size_mismatch (__main__.TestNN) ... ok
test_bce_with_logits_broadcasts_pos_weights (__main__.TestNN) ... ok
test_bce_with_logits_broadcasts_weights (__main__.TestNN) ... ok
test_bce_with_logits_gives_same_result_as_sigmoid_and_bce_loss (__main__.TestNN) ... ok
test_bce_with_logits_gives_same_result_as_sigmoid_and_bce_loss_large_tensors_with_grad (__main__.TestNN) ... ok
test_bce_with_logits_has_correct_grad_at_zero (__main__.TestNN) ... ok
test_bce_with_logits_ones_in_pos_weights_are_the_same_as_none (__main__.TestNN) ... ok
test_bce_with_logits_raises_if_target_and_input_are_different_size (__main__.TestNN) ... ok
test_bce_with_logits_stability (__main__.TestNN) ... ok
test_bce_with_logits_with_pos_weight_has_correct_grad_at_zero (__main__.TestNN) ... ok
test_bilinear (__main__.TestNN) ... ok
test_bilinear_broadcasting (__main__.TestNN) ... ok
test_bilinear_no_bias (__main__.TestNN) ... ok
test_broadcast_double_backwards_gpu (__main__.TestNN) ... skipped "test doesn't currently work on the ROCm stack"
test_broadcast_no_grad (__main__.TestNN) ... libibverbs: Warning: couldn't open config directory '/etc/libibverbs.d'.
ok
test_broadcast_not_requiring_grad (__main__.TestNN) ... ok
test_buffer_not_persistent (__main__.TestNN) ... ok
test_buffer_not_persistent_assign (__main__.TestNN) ... ok
test_buffer_not_persistent_del (__main__.TestNN) ... ok
test_buffer_not_persistent_load (__main__.TestNN) ... ok
test_buffer_not_persistent_overwrite (__main__.TestNN) ... ok
test_buffers_and_named_buffers (__main__.TestNN) ... ok
test_caching_parametrization (__main__.TestNN)
Test the caching system of a parametrization ... ok
test_call_supports_python_dict_output (__main__.TestNN) ... /opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py:903: UserWarning: Using non-full backward hooks on a Module that does not return a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_output. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using non-full backward hooks on a Module that does not return a "
ok
test_channel_shuffle (__main__.TestNN) ... test_nn.py:8964: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /root/pytorch/c10/core/TensorImpl.h:965.)
  y = F.channel_shuffle(x, 2)
ok
test_children (__main__.TestNN) ... ok
test_clip_grad_norm (__main__.TestNN) ... ok
test_clip_grad_value (__main__.TestNN) ... ok
test_compute_nparams_to_prune (__main__.TestNN)
Test that requested pruning `amount` gets translated into the ... ok
test_container_copy (__main__.TestNN) ... ok
test_conv_backcompat (__main__.TestNN) ... ok
test_conv_cudnn_memory_layout_dominance (__main__.TestNN) ... expected failure
test_conv_double_backward (__main__.TestNN) ... ok
test_conv_double_backward_cuda (__main__.TestNN) ... ok
test_conv_double_backward_groups (__main__.TestNN) ... ok
test_conv_double_backward_no_bias (__main__.TestNN) ... ok
test_conv_double_backward_stride (__main__.TestNN) ... ok
test_conv_modules_raise_error_on_incorrect_input_size (__main__.TestNN) ... ok
test_conv_padding_mode (__main__.TestNN) ... ok
test_conv_shapecheck (__main__.TestNN) ... ok
test_conv_tbc (__main__.TestNN) ... ok
test_convert_sync_batchnorm (__main__.TestNN) ... ok
test_cosine_embedding_loss_invalid_target_shape (__main__.TestNN) ... ok
test_cosine_embedding_loss_margin_no_reduce (__main__.TestNN) ... ok
test_cosine_embedding_loss_no_reduce (__main__.TestNN) ... ok
test_cosine_embedding_loss_with_diff_type (__main__.TestNN) ... ok
test_cosine_similarity (__main__.TestNN) ... ok
test_cross_entropy_loss (__main__.TestNN) ... ok
test_cudnn_non_contiguous (__main__.TestNN) ... ok
test_cudnn_noncontiguous_weight (__main__.TestNN) ... ok
test_cudnn_rnn_dropout_states_device (__main__.TestNN) ... ok
test_cudnn_weight_format (__main__.TestNN) ... skipped "test doesn't currently work on the ROCm stack"
test_cudnn_weight_tying (__main__.TestNN) ... ok
test_custom_from_mask_pruning (__main__.TestNN)
Test that the CustomFromMask is capable of receiving ... ok
test_dir (__main__.TestNN) ... ok
test_dir_digit (__main__.TestNN) ... ok
test_dtype_parametrization (__main__.TestNN)
Test a case that is not allowed when removing a parametrization ... ok
test_elu_inplace_gradgrad (__main__.TestNN) ... ok
test_elu_inplace_view (__main__.TestNN) ... ok
test_embedding_from_pretrained (__main__.TestNN) ... ok
test_embedding_from_pretrained_options (__main__.TestNN) ... ok
test_embedding_from_pretrained_padding_idx (__main__.TestNN) ... ok
test_embedding_functional (__main__.TestNN) ... ok
test_embedding_max_norm (__main__.TestNN) ... ok
test_embedding_max_norm_unsorted_repeating_indices (__main__.TestNN) ... ok
test_embedding_sparse_basic (__main__.TestNN) ... ok
test_embedding_sparse_empty_tensor (__main__.TestNN) ... ok
test_embeddingbag_from_pretrained (__main__.TestNN) ... ok
test_embeddingbag_from_pretrained_options (__main__.TestNN) ... ok
test_errors_parametrization (__main__.TestNN) ... ok
test_fb_fc_packed (__main__.TestNN) ... ok
test_flatten (__main__.TestNN) ... ok
test_fold_invalid_arg (__main__.TestNN) ... ok
test_functional_grad_conv (__main__.TestNN) ... ok
test_gaussian_nll_loss_args (__main__.TestNN) ... ok
test_gaussian_nll_loss_reduction_modes (__main__.TestNN) ... ok
test_gelu (__main__.TestNN) ... /opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:380: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
ok
test_getattr_with_property (__main__.TestNN) ... expected failure
test_global_pruning (__main__.TestNN)
Test that global l1 unstructured pruning over 2 parameters removes ... ok
test_global_pruning_importance_scores (__main__.TestNN)
Test that global l1 unstructured pruning over 2 parameters removes ... ok
test_grad_conv1d_input (__main__.TestNN) ... ok
test_grad_conv1d_weight (__main__.TestNN) ... ok
test_grad_conv2d_input (__main__.TestNN) ... ok
test_grad_conv2d_weight (__main__.TestNN) ... ok
test_grad_conv3d_input (__main__.TestNN) ... ok
test_grad_conv3d_weight (__main__.TestNN) ... ok
test_grid_sample (__main__.TestNN) ... ok
test_grid_sample_3d (__main__.TestNN) ... ok
test_grid_sample_error_checking (__main__.TestNN) ... /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3932: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  "Default grid_sample and affine_grid behavior has changed "
ok
test_grouped_conv_cudnn_nhwc_support (__main__.TestNN) ... skipped "test doesn't currently work on the ROCm stack"
test_hardtanh_backward (__main__.TestNN) ... ok
test_hardtanh_inplace_gradgrad (__main__.TestNN) ... ok
test_hook_backward_size (__main__.TestNN) ... ok
test_hook_backward_writeable (__main__.TestNN) ... /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:1791: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py:938: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
ok
test_hook_cpp (__main__.TestNN) ... ok
test_hook_extra_input (__main__.TestNN) ... ok
test_hook_forward_preforward_writable (__main__.TestNN) ... /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:1791: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:1791: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
ok
test_hook_inplace (__main__.TestNN) ... ok
test_hook_invalid_outputs (__main__.TestNN) ... /opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py:938: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py:938: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
ok
test_hook_non_full_warning (__main__.TestNN) ... ok
test_hook_requires_grad (__main__.TestNN) ... ok
test_hooks (__main__.TestNN) ... /opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py:938: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py:938: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py:938: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py:938: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py:938: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
ok
test_huber_loss_invalid_delta (__main__.TestNN) ... ok
test_identity_pruning (__main__.TestNN)
Test that a mask of 1s does not change forward or backward. ... ok
test_initialization_parametrization (__main__.TestNN)
Test that it is possible to initialize a parametrization when it ... ok
test_inplace_thnn (__main__.TestNN) ... ok
test_interpolate (__main__.TestNN) ... /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
ok
test_interpolate_bicubic_2d (__main__.TestNN) ... ok
test_interpolate_bicubic_2d_alert_nondeterministic_cuda (__main__.TestNN) ... ok
test_interpolate_bicubic_2d_cuda (__main__.TestNN) ... ok
test_interpolate_bicubic_2d_zero_dim (__main__.TestNN) ... ok
test_interpolate_bicubic_2d_zero_dim_cuda (__main__.TestNN) ... ok
test_interpolate_bicubic_scale_2d (__main__.TestNN) ... ok
test_interpolate_bicubic_scale_2d_cuda (__main__.TestNN) ... ok
test_interpolate_bicubic_scale_tuple_shared_2d (__main__.TestNN) ... ok
test_interpolate_bicubic_scale_tuple_shared_2d_cuda (__main__.TestNN) ... ok
test_interpolate_bicubic_scale_tuple_skewed_2d (__main__.TestNN) ... ok
test_interpolate_bicubic_scale_tuple_skewed_2d_align_corners (__main__.TestNN) ... ok
test_interpolate_bicubic_scale_tuple_skewed_2d_align_corners_cuda (__main__.TestNN) ... ok
test_interpolate_bicubic_scale_tuple_skewed_2d_cuda (__main__.TestNN) ... ok
test_interpolate_bicubic_tuple_2d (__main__.TestNN) ... ok
test_interpolate_bicubic_tuple_2d_align_corners (__main__.TestNN) ... ok
test_interpolate_bicubic_tuple_2d_align_corners_cuda (__main__.TestNN) ... ok
test_interpolate_bicubic_tuple_2d_cuda (__main__.TestNN) ... ok
test_interpolate_bilinear_2d (__main__.TestNN) ... ok
test_interpolate_bilinear_2d_alert_nondeterministic_cuda (__main__.TestNN) ... ok
test_interpolate_bilinear_2d_cuda (__main__.TestNN) ... ok
test_interpolate_bilinear_2d_zero_dim (__main__.TestNN) ... ok
test_interpolate_bilinear_2d_zero_dim_cuda (__main__.TestNN) ... ok
test_interpolate_bilinear_scale_2d (__main__.TestNN) ... ok
test_interpolate_bilinear_scale_2d_cuda (__main__.TestNN) ... ok
test_interpolate_bilinear_scale_tuple_shared_2d (__main__.TestNN) ... ok
test_interpolate_bilinear_scale_tuple_shared_2d_cuda (__main__.TestNN) ... ok
test_interpolate_bilinear_scale_tuple_skewed_2d (__main__.TestNN) ... ok
test_interpolate_bilinear_scale_tuple_skewed_2d_align_corners (__main__.TestNN) ... ok
test_interpolate_bilinear_scale_tuple_skewed_2d_align_corners_cuda (__main__.TestNN) ... ok
test_interpolate_bilinear_scale_tuple_skewed_2d_cuda (__main__.TestNN) ... ok
test_interpolate_bilinear_tuple_2d (__main__.TestNN) ... ok
test_interpolate_bilinear_tuple_2d_align_corners (__main__.TestNN) ... ok
test_interpolate_bilinear_tuple_2d_align_corners_cuda (__main__.TestNN) ... ok
test_interpolate_bilinear_tuple_2d_cuda (__main__.TestNN) ... ok
test_interpolate_illegal_memory_access (__main__.TestNN) ... ok
test_interpolate_linear_1d (__main__.TestNN) ... ok
test_interpolate_linear_1d_alert_nondeterministic_cuda (__main__.TestNN) ... ok
test_interpolate_linear_1d_align_corners (__main__.TestNN) ... ok
test_interpolate_linear_1d_align_corners_cuda (__main__.TestNN) ... ok
test_interpolate_linear_1d_cuda (__main__.TestNN) ... ok
test_interpolate_linear_1d_zero_dim (__main__.TestNN) ... ok
test_interpolate_linear_1d_zero_dim_cuda (__main__.TestNN) ... ok
test_interpolate_linear_scale_1d (__main__.TestNN) ... ok
test_interpolate_linear_scale_1d_align_corners (__main__.TestNN) ... ok
test_interpolate_linear_scale_1d_align_corners_cuda (__main__.TestNN) ... ok
test_interpolate_linear_scale_1d_cuda (__main__.TestNN) ... ok
test_interpolate_linear_tuple_1d (__main__.TestNN) ... ok
test_interpolate_linear_tuple_1d_cuda (__main__.TestNN) ... ok
test_interpolate_nearest_1d (__main__.TestNN) ... ok
test_interpolate_nearest_1d_cuda (__main__.TestNN) ... ok
test_interpolate_nearest_1d_zero_dim (__main__.TestNN) ... ok
test_interpolate_nearest_1d_zero_dim_cuda (__main__.TestNN) ... ok
test_interpolate_nearest_2d (__main__.TestNN) ... ok
test_interpolate_nearest_2d_cuda (__main__.TestNN) ... ok
test_interpolate_nearest_2d_launch_configs (__main__.TestNN) ... ok
test_interpolate_nearest_2d_launch_configs_cuda (__main__.TestNN) ... ok
test_interpolate_nearest_2d_zero_dim (__main__.TestNN) ... ok
test_interpolate_nearest_2d_zero_dim_cuda (__main__.TestNN) ... ok
test_interpolate_nearest_3d (__main__.TestNN) ... ok
test_interpolate_nearest_3d_cuda (__main__.TestNN) ... ok
test_interpolate_nearest_3d_zero_dim (__main__.TestNN) ... ok
test_interpolate_nearest_3d_zero_dim_cuda (__main__.TestNN) ... ok
test_interpolate_nearest_scale_1d (__main__.TestNN) ... ok
test_interpolate_nearest_scale_1d_cuda (__main__.TestNN) ... ok
test_interpolate_nearest_scale_2d (__main__.TestNN) ... ok
test_interpolate_nearest_scale_2d_cuda (__main__.TestNN) ... ok
test_interpolate_nearest_scale_3d (__main__.TestNN) ... ok
test_interpolate_nearest_scale_3d_cuda (__main__.TestNN) ... ok
test_interpolate_nearest_tuple_1d (__main__.TestNN) ... ok
test_interpolate_nearest_tuple_1d_cuda (__main__.TestNN) ... ok
test_interpolate_nearest_tuple_2d (__main__.TestNN) ... ok
test_interpolate_nearest_tuple_2d_cuda (__main__.TestNN) ... ok
test_interpolate_nearest_tuple_3d (__main__.TestNN) ... ok
test_interpolate_nearest_tuple_3d_cuda (__main__.TestNN) ... ok
test_interpolate_trilinear_3d (__main__.TestNN) ... ok
test_interpolate_trilinear_3d_alert_nondeterministic_cuda (__main__.TestNN) ... ok
test_interpolate_trilinear_3d_cuda (__main__.TestNN) ... ok
test_interpolate_trilinear_3d_zero_dim (__main__.TestNN) ... ok
test_interpolate_trilinear_3d_zero_dim_cuda (__main__.TestNN) ... ok
test_interpolate_trilinear_scale_3d (__main__.TestNN) ... ok
test_interpolate_trilinear_scale_3d_align_corners (__main__.TestNN) ... ok
test_interpolate_trilinear_scale_3d_align_corners_cuda (__main__.TestNN) ... ok
test_interpolate_trilinear_scale_3d_cuda (__main__.TestNN) ... ok
test_interpolate_trilinear_tuple_3d (__main__.TestNN) ... ok
test_interpolate_trilinear_tuple_3d_align_corners (__main__.TestNN) ... ok
test_interpolate_trilinear_tuple_3d_align_corners_cuda (__main__.TestNN) ... ok
test_interpolate_trilinear_tuple_3d_cuda (__main__.TestNN) ... ok
test_invalid_conv1d (__main__.TestNN) ... ok
test_invalid_conv2d (__main__.TestNN) ... ok
test_invalid_conv3d (__main__.TestNN) ... ok
test_invalid_dropout_p (__main__.TestNN) ... ok
test_kl_div_log_softmax_target (__main__.TestNN) ... ok
test_kl_div_with_diff_type (__main__.TestNN) ... /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
ok
test_kl_div_with_diff_type_log_target (__main__.TestNN) ... /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
ok
test_l1_loss_correct (__main__.TestNN) ... ok
test_l1_unstructured_pruning (__main__.TestNN)
Test that l1 unstructured pruning actually removes the lowest ... ok
test_l1_unstructured_pruning_with_importance_scores (__main__.TestNN)
Test that l1 unstructured pruning actually removes the lowest ... ok
test_layer_norm_grads_with_create_graph_flag (__main__.TestNN) ... ok
test_linear_broadcasting (__main__.TestNN) ... ok
test_ln_structured_pruning (__main__.TestNN)
Check Ln structured pruning by hand. ... ok
test_ln_structured_pruning_importance_scores (__main__.TestNN)
Check Ln structured pruning by hand. ... ok
test_load_state_dict (__main__.TestNN) ... ok
test_load_state_dict_BC (__main__.TestNN) ... ok
test_load_state_dict_custom (__main__.TestNN) ... ok
test_load_state_dict_ref_cycle (__main__.TestNN) ... ok
test_log_softmax_cpu (__main__.TestNN) ... ok
test_log_softmax_dim0 (__main__.TestNN) ... ok
test_log_softmax_dim0_cuda (__main__.TestNN) ... ok
test_log_softmax_dim3 (__main__.TestNN) ... ok
test_log_softmax_dim3_cuda (__main__.TestNN) ... ok
test_log_softmax_lastdim (__main__.TestNN) ... ok
test_log_softmax_lastdim_cuda (__main__.TestNN) ... ok
test_log_softmax_scalar (__main__.TestNN) ... ok
test_log_softmax_scalar_cuda (__main__.TestNN) ... ok
test_log_softmax_spatial (__main__.TestNN) ... ok
test_log_softmax_spatial_cuda (__main__.TestNN) ... ok
test_log_softmax_spatial_special (__main__.TestNN) ... ok
test_log_softmax_spatial_special_cuda (__main__.TestNN) ... ok
test_loss_equal_input_target_shape (__main__.TestNN) ... test_nn.py:5154: UserWarning: Using a target size (torch.Size([5, 3])) that is different to the input size (torch.Size([3, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y: F.mse_loss(x, y),
test_nn.py:5155: UserWarning: Using a target size (torch.Size([5, 3])) that is different to the input size (torch.Size([3, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y: F.l1_loss(x, y),
test_nn.py:5156: UserWarning: Using a target size (torch.Size([5, 3])) that is different to the input size (torch.Size([3, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y: F.smooth_l1_loss(x, y),
test_nn.py:5157: UserWarning: Using a target size (torch.Size([5, 3])) that is different to the input size (torch.Size([3, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y: F.huber_loss(x, y),
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2692: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
ok
test_margin_ranking_loss_margin_no_reduce (__main__.TestNN) ... ok
test_margin_ranking_loss_no_reduce (__main__.TestNN) ... ok
test_mismatch_shape_conv2d (__main__.TestNN) ... ok
test_module_apply_inplace_op (__main__.TestNN) ... ok
test_module_backcompat (__main__.TestNN) ... ok
test_module_to_argparse (__main__.TestNN) ... ok
test_modules (__main__.TestNN) ... ok
test_move_sparse_half_embedding (__main__.TestNN) ... ok
test_mse_loss_size_warning (__main__.TestNN) ... ok
test_multihead_attention (__main__.TestNN) ... /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4841: UserWarning: Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:4858: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
ok
test_multihead_attn_3d_attn_mask (__main__.TestNN) ... ok
test_multihead_attn_no_bias (__main__.TestNN) ... ok
test_multimarginloss_1d_input_0d_target_no_reduce (__main__.TestNN) ... ok
test_multimarginloss_1d_input_0d_target_no_reduce_cuda (__main__.TestNN) ... ok
test_multiple_pruning_calls (__main__.TestNN) ... ok
test_named_children (__main__.TestNN) ... ok
test_named_modules (__main__.TestNN) ... ok
test_nnpack_conv (__main__.TestNN) ... ok
test_no_grad (__main__.TestNN) ... ok
test_non_leaf_parameters (__main__.TestNN) ... ok
test_noncontig_conv_grad_cuda (__main__.TestNN) ... ok
test_normalize (__main__.TestNN) ... ok
test_overwrite_module_params_on_conversion (__main__.TestNN) ... ok
test_pack_padded_sequence (__main__.TestNN) ... ok
test_pack_sequence (__main__.TestNN) ... ok
test_pack_sequence_batch_sizes_throw (__main__.TestNN) ... ok
test_pad_scalar_error (__main__.TestNN) ... ok
test_pad_sequence (__main__.TestNN) ... ok
test_pairwise_distance (__main__.TestNN) ... ok
test_parameter_assignment (__main__.TestNN) ... ok
test_parameterlistdict_pickle (__main__.TestNN) ... ok
test_parameterlistdict_setting_attributes (__main__.TestNN) ... ok
test_parameters_and_named_parameters (__main__.TestNN) ... ok
test_parameters_to_vector (__main__.TestNN) ... ok
test_partial_flat_weights (__main__.TestNN) ... ok
test_pdist (__main__.TestNN) ... ok
test_pdist_cpu_gradgrad_unimplemented (__main__.TestNN) ... expected failure
test_pdist_cuda_gradgrad_unimplemented (__main__.TestNN) ... expected failure
test_pdist_empty_col (__main__.TestNN) ... ok
test_pdist_empty_row (__main__.TestNN) ... ok
test_pdist_zeros (__main__.TestNN)
Test that grad is still valid when dist is 0 ... ok
test_pixel_shuffle_unshuffle (__main__.TestNN) ... ok
test_pointwise_loss_broadcast (__main__.TestNN) ... test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8182: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'mse_loss': lambda x, y, r: F.mse_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8183: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'l1_loss': lambda x, y, r: F.l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8184: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'smooth_l1_loss': lambda x, y, r: F.smooth_l1_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
test_nn.py:8185: UserWarning: Using a target size (torch.Size([2, 10])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  'huber_loss': lambda x, y, r: F.huber_loss(x, y, reduction=r),
ok
test_pointwise_loss_target_grad_none_reduction (__main__.TestNN) ... ok
test_poisson_nll_loss_reduction_modes (__main__.TestNN) ... ok
test_projections_errors_on_gru_and_rnn (__main__.TestNN) ... ok
test_projections_lstm_args_check (__main__.TestNN) ... ok
test_projections_lstm_check_device (__main__.TestNN) ... ok
test_projections_lstm_initial_hidden_state (__main__.TestNN) ... ok
test_prune (__main__.TestNN) ... ok
test_prune_importance_scores (__main__.TestNN) ... ok
test_prune_importance_scores_mimic_default (__main__.TestNN) ... ok
test_pruning_container (__main__.TestNN) ... ok
test_pruning_container_compute_mask (__main__.TestNN)
Test `compute_mask` of pruning container with a known `t` and ... ok
test_pruning_id_consistency (__main__.TestNN)
Test that pruning doesn't change the id of the parameters, which ... ok
test_pruning_rollback (__main__.TestNN)
Test that if something fails when the we try to compute the mask, ... ok
test_pruning_serialization_model (__main__.TestNN) ... ok
test_pruning_serialization_state_dict (__main__.TestNN) ... ok
test_random_pruning (__main__.TestNN) ... ok
test_random_pruning_0perc (__main__.TestNN)
Test that a mask of 1s does not change forward or backward. ... ok
test_random_pruning_forward (__main__.TestNN)
check forward with mask (by hand). ... ok
test_random_pruning_new_weight (__main__.TestNN)
Test that module.name now contains a pruned version of ... ok
test_random_pruning_orig (__main__.TestNN)
Test that original tensor is correctly stored in 'orig' ... ok
test_random_pruning_pickle (__main__.TestNN) ... ok
test_random_pruning_sizes (__main__.TestNN)
Test that the new parameters and buffers created by the pruning ... ok
test_random_structured_pruning_amount (__main__.TestNN) ... ok
test_register_and_remove_buffer_parametrization (__main__.TestNN)
Test that it is possible to add and remove parametrizations on buffers ... ok
test_register_and_remove_parametrization (__main__.TestNN)
Test that it is possible to add a few parametrizations ... ok
test_register_buffer_allows_overwriting_with_same_name (__main__.TestNN) ... ok
test_register_buffer_raises_error_if_attr_exists (__main__.TestNN) ... ok
test_register_buffer_raises_error_if_name_is_not_string (__main__.TestNN) ... ok
test_register_buffer_raises_error_if_not_tensor (__main__.TestNN) ... ok
test_register_parameter_allows_overwriting_with_same_name (__main__.TestNN) ... ok
test_register_parameter_raises_error_if_attr_exists (__main__.TestNN) ... ok
test_register_parameter_raises_error_if_name_is_not_string (__main__.TestNN) ... ok
test_relu_inplace_view (__main__.TestNN) ... ok
test_remove_pruning (__main__.TestNN)
`prune.remove` removes the hook and the reparametrization ... ok
test_remove_pruning_exception (__main__.TestNN)
Removing from an unpruned tensor throws an assertion error ... ok
test_remove_pruning_forward (__main__.TestNN)
Remove pruning and check forward is unchanged from previous ... ok
test_repr (__main__.TestNN) ... ok
test_requires_grad_ (__main__.TestNN) ... ok
test_rnn_args_check (__main__.TestNN) ... ok
test_rnn_check_device (__main__.TestNN) ... ok
test_rnn_initial_hidden_state (__main__.TestNN) ... ok
test_rnn_pruning (__main__.TestNN) ... ok
test_rnn_weight_norm (__main__.TestNN) ... ok
test_serialization_parametrization (__main__.TestNN)
Test that it is possible to serialize a parametrized model via state_dict ... ok
test_share_memory (__main__.TestNN) ... ok
test_smoothl1loss_negative_beta_not_supported (__main__.TestNN) ... ok
test_softmax_functional_dim0 (__main__.TestNN) ... ok
test_softmax_functional_dim0_cuda (__main__.TestNN) ... skipped 'Excluded from CUDA tests'
test_softmax_functional_dim3 (__main__.TestNN) ... ok
test_softmax_functional_dim3_cuda (__main__.TestNN) ... skipped 'Excluded from CUDA tests'
test_softmax_functional_scalar (__main__.TestNN) ... ok
test_softmax_functional_scalar_cuda (__main__.TestNN) ... skipped 'Excluded from CUDA tests'
test_softmax_lastdim (__main__.TestNN) ... ok
test_softmax_lastdim_cuda (__main__.TestNN) ... ok
test_softmax_lastdim_dtype (__main__.TestNN) ... ok
test_softmax_lastdim_dtype_cuda (__main__.TestNN) ... skipped 'Excluded from CUDA tests'
test_softmax_spatial (__main__.TestNN) ... ok
test_softmax_spatial_cuda (__main__.TestNN) ... ok
test_softmax_spatial_dtype (__main__.TestNN) ... ok
test_softmax_spatial_dtype_cuda (__main__.TestNN) ... skipped 'Excluded from CUDA tests'
test_softmax_spatial_special (__main__.TestNN) ... ok
test_softmax_spatial_special_cuda (__main__.TestNN) ... ok
test_softmin (__main__.TestNN) ... ok
test_spectral_norm (__main__.TestNN) ... ok
test_spectral_norm_dim (__main__.TestNN) ... ok
test_spectral_norm_forward (__main__.TestNN) ... ok
test_spectral_norm_load_state_dict (__main__.TestNN) ... ok
test_spectral_norm_pickle (__main__.TestNN) ... ok
test_state_dict (__main__.TestNN) ... ok
test_threshold_int (__main__.TestNN) ... ok
test_to (__main__.TestNN) ... ok
test_transformer_args_check (__main__.TestNN) ... ok
test_transformer_layer_args_check (__main__.TestNN) ... ok
test_transformerdecoder (__main__.TestNN) ... ok
test_transformerdecoderlayer (__main__.TestNN) ... ok
test_transformerdecoderlayer_gelu (__main__.TestNN) ... ok
test_transformerencoder (__main__.TestNN) ... ok
test_transformerencoderlayer (__main__.TestNN) ... ok
test_transformerencoderlayer_gelu (__main__.TestNN) ... ok
test_triplet_margin_loss (__main__.TestNN) ... ok
test_triplet_margin_loss_no_reduce (__main__.TestNN) ... ok
test_triplet_margin_loss_swap (__main__.TestNN) ... ok
test_triplet_margin_loss_swap_no_reduce (__main__.TestNN) ... ok
test_type (__main__.TestNN) ... ok
test_unflatten (__main__.TestNN) ... ok
test_unflatten_invalid_arg (__main__.TestNN) ... ok
test_unfold_invalid_arg (__main__.TestNN) ... ok
test_unstructured_pruning_same_magnitude (__main__.TestNN)
Since it may happen that the tensor to prune has entries with the ... ok
test_upsamplingBicubic2d (__main__.TestNN) ... /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
ok
test_upsamplingBilinear2d (__main__.TestNN) ... ok
test_upsamplingBilinear2d_spatial_invariance (__main__.TestNN) ... ok
test_upsamplingLinear1d (__main__.TestNN) ... ok
test_upsamplingLinear1d_spatial_invariance (__main__.TestNN) ... ok
test_upsamplingNearest1d (__main__.TestNN) ... ok
test_upsamplingNearest2d (__main__.TestNN) ... ok
test_upsamplingNearest3d (__main__.TestNN) ... ok
test_upsamplingTrilinear3d (__main__.TestNN) ... /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
ok
test_upsamplingTrilinear3d_spatial_invariance (__main__.TestNN) ... ok
test_upsampling_not_recompute_scale_factor (__main__.TestNN) ... /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3608: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
ok
test_validate_pruning_amount (__main__.TestNN)
Tests the second util function that validates the pruning ... ok
test_validate_pruning_amount_init (__main__.TestNN)
Test the first util function that validates the pruning ... ok
test_vector_to_parameters (__main__.TestNN) ... ok
test_weight_norm (__main__.TestNN) ... ok
test_weight_norm_pickle (__main__.TestNN) ... ok
test_zero_grad (__main__.TestNN) ... ok
test_AdaptiveMaxPool1d_indices_cpu_float32 (__main__.TestNNDeviceTypeCPU) ... ok
test_AdaptiveMaxPool2d_indices_cpu_float32 (__main__.TestNNDeviceTypeCPU) ... ok
test_AdaptiveMaxPool3d_indices_cpu_float32 (__main__.TestNNDeviceTypeCPU) ... ok
test_AvgPool2d_empty_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_AvgPool3d_backward_after_cat_dim1_device_cpu (__main__.TestNNDeviceTypeCPU) ... skipped 'Only runs on cuda'
test_BatchNorm_empty_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_CTCLoss_cudnn_cpu (__main__.TestNNDeviceTypeCPU) ... skipped 'Only runs on cuda'
test_CTCLoss_empty_target_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_Conv2d_naive_groups_cpu_float32 (__main__.TestNNDeviceTypeCPU) ... ok
test_Conv2d_size_1_kernel_cpu (__main__.TestNNDeviceTypeCPU) ... skipped 'Only runs on cuda'
test_ConvTranspose2d_size_1_kernel_cpu (__main__.TestNNDeviceTypeCPU) ... skipped 'Only runs on cuda'
test_ConvTranspose3d_size_1_kernel_cpu (__main__.TestNNDeviceTypeCPU) ... skipped 'Only runs on cuda'
test_Dropout2d_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_Dropout3d_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_Dropout_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_EmbeddingBag_empty_per_sample_weights_and_offsets_cpu_int32_float32 (__main__.TestNNDeviceTypeCPU) ... ok
test_EmbeddingBag_empty_per_sample_weights_and_offsets_cpu_int32_float64 (__main__.TestNNDeviceTypeCPU) ... ok
test_EmbeddingBag_empty_per_sample_weights_and_offsets_cpu_int64_float32 (__main__.TestNNDeviceTypeCPU) ... ok
test_EmbeddingBag_empty_per_sample_weights_and_offsets_cpu_int64_float64 (__main__.TestNNDeviceTypeCPU) ... ok
test_EmbeddingBag_per_sample_weights_and_new_offsets_cpu_int32_float32 (__main__.TestNNDeviceTypeCPU) ... ok
test_EmbeddingBag_per_sample_weights_and_new_offsets_cpu_int32_float64 (__main__.TestNNDeviceTypeCPU) ... ok
test_EmbeddingBag_per_sample_weights_and_new_offsets_cpu_int64_float32 (__main__.TestNNDeviceTypeCPU) ... ok
test_EmbeddingBag_per_sample_weights_and_new_offsets_cpu_int64_float64 (__main__.TestNNDeviceTypeCPU) ... ok
test_EmbeddingBag_per_sample_weights_and_no_offsets_cpu_int32_float32 (__main__.TestNNDeviceTypeCPU) ... ok
test_EmbeddingBag_per_sample_weights_and_no_offsets_cpu_int32_float64 (__main__.TestNNDeviceTypeCPU) ... ok
test_EmbeddingBag_per_sample_weights_and_no_offsets_cpu_int64_float32 (__main__.TestNNDeviceTypeCPU) ... ok
test_EmbeddingBag_per_sample_weights_and_no_offsets_cpu_int64_float64 (__main__.TestNNDeviceTypeCPU) ... ok
test_EmbeddingBag_per_sample_weights_and_offsets_cpu_int32_float32 (__main__.TestNNDeviceTypeCPU) ... ok
test_EmbeddingBag_per_sample_weights_and_offsets_cpu_int32_float64 (__main__.TestNNDeviceTypeCPU) ... ok
test_EmbeddingBag_per_sample_weights_and_offsets_cpu_int64_float32 (__main__.TestNNDeviceTypeCPU) ... ok
test_EmbeddingBag_per_sample_weights_and_offsets_cpu_int64_float64 (__main__.TestNNDeviceTypeCPU) ... ok
test_EmbeddingBag_per_sample_weights_failures_cpu_int32 (__main__.TestNNDeviceTypeCPU) ... ok
test_EmbeddingBag_per_sample_weights_failures_cpu_int64 (__main__.TestNNDeviceTypeCPU) ... ok
test_GroupNorm_empty_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_GroupNorm_general_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_GroupNorm_raises_error_if_one_value_per_group_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_InstanceNorm1d_general_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_InstanceNorm2d_general_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_InstanceNorm3d_general_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_LayerNorm_general_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_MarginLoss_empty_cpu_float32 (__main__.TestNNDeviceTypeCPU) ... ok
test_MarginLoss_empty_cpu_float64 (__main__.TestNNDeviceTypeCPU) ... ok
test_MaxPool1d_indices_cpu_float32 (__main__.TestNNDeviceTypeCPU) ... ok
test_MaxPool2d_indices_cpu_float32 (__main__.TestNNDeviceTypeCPU) ... ok
test_MaxPool3d_indices_cpu_float32 (__main__.TestNNDeviceTypeCPU) ... ok
test_ReflectionPad_empty_cpu_complex64 (__main__.TestNNDeviceTypeCPU) ... ok
test_ReflectionPad_empty_cpu_float32 (__main__.TestNNDeviceTypeCPU) ... ok
test_ReplicationPad1d_large_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_ReplicationPad2d_large_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_ReplicationPad3d_large_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_ReplicationPad_empty_cpu_complex128 (__main__.TestNNDeviceTypeCPU) ... ok
test_ReplicationPad_empty_cpu_float64 (__main__.TestNNDeviceTypeCPU) ... ok
test_Unfold_empty_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_activations_bfloat16_cpu (__main__.TestNNDeviceTypeCPU) ... skipped 'Only runs on cuda'
test_adaptive_avg_pool2d_output_size_one_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_adaptive_pooling_no_suppot_input_cpu_int16 (__main__.TestNNDeviceTypeCPU) ... ok
test_adaptive_pooling_no_suppot_input_cpu_int32 (__main__.TestNNDeviceTypeCPU) ... ok
test_adaptive_pooling_no_suppot_input_cpu_int64 (__main__.TestNNDeviceTypeCPU) ... ok
test_adaptive_pooling_no_suppot_input_cpu_int8 (__main__.TestNNDeviceTypeCPU) ... ok
test_adaptive_pooling_no_suppot_input_cpu_uint8 (__main__.TestNNDeviceTypeCPU) ... ok
test_affine_2d_rotate0_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_affine_2d_rotate45_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_affine_2d_rotate90_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_affine_2d_rotateRandom_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_affine_3d_rotateRandom_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_avg_pool2d_nhwc_cpu (__main__.TestNNDeviceTypeCPU) ... skipped 'Only runs on cuda'
test_batchnorm_eval_bfloat16_cpu (__main__.TestNNDeviceTypeCPU) ... skipped 'Only runs on cuda'
test_batchnorm_eval_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_batchnorm_grad_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_batchnorm_simple_average_cpu_float32 (__main__.TestNNDeviceTypeCPU) ... ok
test_batchnorm_update_stats_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_clip_grad_norm_multi_device_cpu (__main__.TestNNDeviceTypeCPU) ... skipped 'Only runs on cuda'
test_contig_wrong_stride_cudnn_cpu (__main__.TestNNDeviceTypeCPU) ... skipped 'Only runs on cuda'
test_convTranspose_empty_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_conv_cudnn_mismatch_memory_format_cpu (__main__.TestNNDeviceTypeCPU) ... skipped 'Only runs on cuda'
test_conv_cudnn_nhwc_cpu_float16 (__main__.TestNNDeviceTypeCPU) ... skipped 'Only runs on cuda'
test_conv_cudnn_nhwc_cpu_float32 (__main__.TestNNDeviceTypeCPU) ... skipped 'Only runs on cuda'
test_conv_cudnn_nhwc_support_cpu_float32 (__main__.TestNNDeviceTypeCPU) ... skipped 'Only runs on cuda'
test_conv_cudnn_nhwc_support_cpu_float64 (__main__.TestNNDeviceTypeCPU) ... skipped 'Only runs on cuda'
test_conv_large_cpu (__main__.TestNNDeviceTypeCPU) ... skipped 'Only runs on cuda'
test_conv_large_nosplit_cpu (__main__.TestNNDeviceTypeCPU) ... skipped 'Only runs on cuda'
test_conv_noncontig_weights_and_bias_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_conv_noncontig_weights_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_conv_transposed_large_cpu (__main__.TestNNDeviceTypeCPU) ... skipped 'Only runs on cuda'
test_convert_conv2d_weight_memory_format_cpu (__main__.TestNNDeviceTypeCPU) ... skipped 'Only runs on cuda'
test_device_mask_cpu (__main__.TestNNDeviceTypeCPU) ... skipped 'Only runs on cuda'
test_elu_inplace_overlap_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_embedding_backward_cpu_float64 (__main__.TestNNDeviceTypeCPU) ... ok
test_embedding_bag_bfloat16_cpu_int32 (__main__.TestNNDeviceTypeCPU) ... skipped 'Only runs on cuda'
test_embedding_bag_bfloat16_cpu_int64 (__main__.TestNNDeviceTypeCPU) ... skipped 'Only runs on cuda'
test_embedding_bag_device_cpu_int32_float32 (__main__.TestNNDeviceTypeCPU) ... ok
test_embedding_bag_device_cpu_int32_float64 (__main__.TestNNDeviceTypeCPU) ... ok
test_embedding_bag_device_cpu_int64_float32 (__main__.TestNNDeviceTypeCPU) ... ok
test_embedding_bag_device_cpu_int64_float64 (__main__.TestNNDeviceTypeCPU) ... ok
test_embedding_bag_empty_input_cpu_int32 (__main__.TestNNDeviceTypeCPU) ... ok
test_embedding_bag_empty_input_cpu_int64 (__main__.TestNNDeviceTypeCPU) ... ok
test_embedding_bag_non_contiguous_weight_cpu_int32_float32 (__main__.TestNNDeviceTypeCPU) ... ok
test_embedding_bag_non_contiguous_weight_cpu_int32_float64 (__main__.TestNNDeviceTypeCPU) ... ok
test_embedding_bag_non_contiguous_weight_cpu_int64_float32 (__main__.TestNNDeviceTypeCPU) ... ok
test_embedding_bag_non_contiguous_weight_cpu_int64_float64 (__main__.TestNNDeviceTypeCPU) ... ok
test_embedding_dense_grad_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_embedding_max_norm_device_cpu_bfloat16 (__main__.TestNNDeviceTypeCPU) ... skipped 'Only runs on cuda'
test_embedding_max_norm_device_cpu_float16 (__main__.TestNNDeviceTypeCPU) ... skipped 'Only runs on cuda'
test_embedding_max_norm_device_cpu_float32 (__main__.TestNNDeviceTypeCPU) ... skipped 'Only runs on cuda'
test_embedding_max_norm_device_cpu_float64 (__main__.TestNNDeviceTypeCPU) ... skipped 'Only runs on cuda'
test_embedding_padding_idx_cpu_float32 (__main__.TestNNDeviceTypeCPU) ... ok
test_embedding_scalar_weight_error_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_empty_dropout_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_fold_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_fractional_max_pool2d_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_fractional_max_pool3d_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_fractional_max_pool_nan_inf_cpu_float32 (__main__.TestNNDeviceTypeCPU) ... ok
test_grid_sample_2d_alert_nondeterministic_cpu (__main__.TestNNDeviceTypeCPU) ... skipped 'Only runs on cuda'
test_grid_sample_3d_alert_nondeterministic_cpu (__main__.TestNNDeviceTypeCPU) ... skipped 'Only runs on cuda'
test_grid_sample_large_cpu (__main__.TestNNDeviceTypeCPU) ... skipped 'Only runs on cuda'
test_grid_sample_large_index_2d_cpu_float32 (__main__.TestNNDeviceTypeCPU) ... ok
test_grid_sample_large_index_2d_cpu_float64 (__main__.TestNNDeviceTypeCPU) ... ok
test_grid_sample_large_index_3d_cpu_float32 (__main__.TestNNDeviceTypeCPU) ... ok
test_grid_sample_large_index_3d_cpu_float64 (__main__.TestNNDeviceTypeCPU) ... ok
test_grid_sample_nan_inf_cpu_float32 (__main__.TestNNDeviceTypeCPU) ... ok
test_grid_sample_nan_inf_cpu_float64 (__main__.TestNNDeviceTypeCPU) ... ok
test_group_convTranspose_empty_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_group_conv_empty_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_gumbel_softmax_cpu_float32 (__main__.TestNNDeviceTypeCPU) ... ok
test_gumbel_softmax_cpu_float64 (__main__.TestNNDeviceTypeCPU) ... ok
test_hardsigmoid_grad_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_hardswish_grad_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_hardswish_inplace_overlap_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_instancenorm_raises_error_if_less_than_one_value_per_channel_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_invalid_reduction_strings_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_leaky_relu_inplace_overlap_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_linear_empty_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_log_softmax_big_cpu_float32 (__main__.TestNNDeviceTypeCPU) ... ok
test_logsigmoid_out_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_max_pool1d_corner_cases_cpu_float32 (__main__.TestNNDeviceTypeCPU) ... ok
test_max_pool1d_corner_cases_cpu_float64 (__main__.TestNNDeviceTypeCPU) ... ok
test_max_pool1d_cpu_float32 (__main__.TestNNDeviceTypeCPU) ... ok
test_max_pool1d_cpu_float64 (__main__.TestNNDeviceTypeCPU) ... ok
test_max_pool1d_errors_cpu_float32 (__main__.TestNNDeviceTypeCPU) ... ok
test_max_pool2d_cpu (__main__.TestNNDeviceTypeCPU) ... skipped 'Only runs on cuda'
test_max_pool2d_indices_cpu (__main__.TestNNDeviceTypeCPU) ... skipped 'Only runs on cuda'
test_max_pool2d_nhwc_cpu (__main__.TestNNDeviceTypeCPU) ... skipped 'Only runs on cuda'
test_max_pool_nan_inf_cpu_float32 (__main__.TestNNDeviceTypeCPU) ... ok
test_maxpool3d_non_square_backward_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_multi_margin_loss_errors_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_multihead_attention_dtype_cpu_float16 (__main__.TestNNDeviceTypeCPU) ... skipped 'Only runs on cuda'
test_multihead_attention_dtype_cpu_float32 (__main__.TestNNDeviceTypeCPU) ... skipped 'Only runs on cuda'
test_multihead_attention_dtype_cpu_float64 (__main__.TestNNDeviceTypeCPU) ... skipped 'Only runs on cuda'
test_nll_loss_empty_tensor_reduction_mean_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_nll_loss_empty_tensor_reduction_none_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_nll_loss_empty_tensor_reduction_sum_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_nll_loss_mismatched_batch_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_nll_loss_out_of_bounds_ignore_index_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_nll_loss_total_weight_is_zero_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_nn_scalars_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_nn_scalars_reductions_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_nonlinearity_propagate_nan_cpu (__main__.TestNNDeviceTypeCPU) ... /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:1780: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:1791: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
ok
test_one_hot_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_overwrite_module_params_on_conversion_cpu_device_cpu (__main__.TestNNDeviceTypeCPU) ... skipped 'Only runs on cuda'
test_pad_cpu_complex128 (__main__.TestNNDeviceTypeCPU) ... ok
test_pad_cpu_float64 (__main__.TestNNDeviceTypeCPU) ... ok
test_pool3d_large_size_int64_cpu (__main__.TestNNDeviceTypeCPU) ... skipped 'Only runs on cuda'
test_pool3d_size_one_feature_dim_cpu (__main__.TestNNDeviceTypeCPU) ... skipped 'Only runs on cuda'
test_pool_invalid_size_cpu_float32 (__main__.TestNNDeviceTypeCPU) ... ok
test_pool_large_size_cpu_float32 (__main__.TestNNDeviceTypeCPU) ... ok
test_pooling_bfloat16_cpu (__main__.TestNNDeviceTypeCPU) ... skipped 'Only runs on cuda'
test_pooling_shape_cpu (__main__.TestNNDeviceTypeCPU)
Test the output shape calculation for pooling functions ... ok
test_pooling_size_empty_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_pooling_zero_stride_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_prelu_backward_32bit_indexing_cpu (__main__.TestNNDeviceTypeCPU) ... skipped 'Only runs on cuda'
test_rnn_fused_cpu_float32 (__main__.TestNNDeviceTypeCPU) ... skipped 'Only runs on cuda'
test_rnn_fused_cpu_float64 (__main__.TestNNDeviceTypeCPU) ... skipped 'Only runs on cuda'
test_rnn_retain_variables_cpu_float64 (__main__.TestNNDeviceTypeCPU) ... ok
test_silu_inplace_overlap_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_smooth_l1_loss_vs_huber_loss_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_softmax_64bit_indexing_cpu (__main__.TestNNDeviceTypeCPU) ... skipped 'Only runs on cuda'
test_softmax_bfloat16_cpu (__main__.TestNNDeviceTypeCPU) ... skipped 'Only runs on cuda'
test_softmax_cpu_float16 (__main__.TestNNDeviceTypeCPU) ... skipped 'Only runs on cuda'
test_softmax_cpu_float32 (__main__.TestNNDeviceTypeCPU) ... skipped 'Only runs on cuda'
test_softmax_results_cpu_float32 (__main__.TestNNDeviceTypeCPU) ... ok
test_softplus_inplace_overlap_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_softshrink_inplace_overlap_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_softshrink_negative_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_threshold_inplace_overlap_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_to_complex_cpu (__main__.TestNNDeviceTypeCPU) ... /opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py:805: UserWarning: Complex modules are a new feature under active development whose design may change, and some modules might not work as expected when using complex tensors as parameters or buffers. Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md if a complex module does not work as expected.
  "Complex modules are a new feature under active development whose design may change, "
/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py:805: UserWarning: Complex modules are a new feature under active development whose design may change, and some modules might not work as expected when using complex tensors as parameters or buffers. Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md if a complex module does not work as expected.
  "Complex modules are a new feature under active development whose design may change, "
/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py:814: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at  /root/pytorch/aten/src/ATen/native/Copy.cpp:219.)
  return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
ok
test_triplet_margin_with_distance_loss_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_triplet_margin_with_distance_loss_default_parity_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_unfold_cpu (__main__.TestNNDeviceTypeCPU) ... ok
test_upsamplingNearest1d_launch_config_cpu (__main__.TestNNDeviceTypeCPU) ... skipped 'Only runs on cuda'
test_upsamplingNearest2d_launch_config_cpu (__main__.TestNNDeviceTypeCPU) ... skipped 'Only runs on cuda'
test_upsamplingNearest2d_launch_fail_cpu (__main__.TestNNDeviceTypeCPU) ... skipped "test doesn't currently work on the ROCm stack"
test_upsamplingNearest2d_launch_rocm_cpu (__main__.TestNNDeviceTypeCPU) ... skipped 'Only runs on cuda'
test_upsamplingNearest3d_launch_config_cpu (__main__.TestNNDeviceTypeCPU) ... skipped 'Only runs on cuda'
test_variable_sequence_cpu_float32 (__main__.TestNNDeviceTypeCPU) ... ok
test_AdaptiveMaxPool1d_indices_cuda_bfloat16 (__main__.TestNNDeviceTypeCUDA) ... ok
test_AdaptiveMaxPool1d_indices_cuda_float16 (__main__.TestNNDeviceTypeCUDA) ... ok
test_AdaptiveMaxPool1d_indices_cuda_float32 (__main__.TestNNDeviceTypeCUDA) ... ok
test_AdaptiveMaxPool1d_indices_cuda_float64 (__main__.TestNNDeviceTypeCUDA) ... ok
test_AdaptiveMaxPool2d_indices_cuda_bfloat16 (__main__.TestNNDeviceTypeCUDA) ... ok
test_AdaptiveMaxPool2d_indices_cuda_float16 (__main__.TestNNDeviceTypeCUDA) ... ok
test_AdaptiveMaxPool2d_indices_cuda_float32 (__main__.TestNNDeviceTypeCUDA) ... ok
test_AdaptiveMaxPool2d_indices_cuda_float64 (__main__.TestNNDeviceTypeCUDA) ... ok
test_AdaptiveMaxPool3d_indices_cuda_bfloat16 (__main__.TestNNDeviceTypeCUDA) ... ok
test_AdaptiveMaxPool3d_indices_cuda_float16 (__main__.TestNNDeviceTypeCUDA) ... ok
test_AdaptiveMaxPool3d_indices_cuda_float32 (__main__.TestNNDeviceTypeCUDA) ... ok
test_AdaptiveMaxPool3d_indices_cuda_float64 (__main__.TestNNDeviceTypeCUDA) ... ok
test_AvgPool2d_empty_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_AvgPool3d_backward_after_cat_dim1_device_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_BatchNorm_empty_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_CTCLoss_cudnn_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_CTCLoss_empty_target_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_Conv2d_naive_groups_cuda_bfloat16 (__main__.TestNNDeviceTypeCUDA) ... ok
test_Conv2d_naive_groups_cuda_float16 (__main__.TestNNDeviceTypeCUDA) ... ok
test_Conv2d_naive_groups_cuda_float32 (__main__.TestNNDeviceTypeCUDA) ... ok
test_Conv2d_naive_groups_cuda_float64 (__main__.TestNNDeviceTypeCUDA) ... ok
test_Conv2d_size_1_kernel_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_ConvTranspose2d_size_1_kernel_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_ConvTranspose3d_size_1_kernel_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_Dropout2d_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_Dropout3d_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_Dropout_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_EmbeddingBag_empty_per_sample_weights_and_offsets_cuda_int32_float16 (__main__.TestNNDeviceTypeCUDA) ... ok
test_EmbeddingBag_empty_per_sample_weights_and_offsets_cuda_int32_float32 (__main__.TestNNDeviceTypeCUDA) ... ok
test_EmbeddingBag_empty_per_sample_weights_and_offsets_cuda_int32_float64 (__main__.TestNNDeviceTypeCUDA) ... ok
test_EmbeddingBag_empty_per_sample_weights_and_offsets_cuda_int64_float16 (__main__.TestNNDeviceTypeCUDA) ... ok
test_EmbeddingBag_empty_per_sample_weights_and_offsets_cuda_int64_float32 (__main__.TestNNDeviceTypeCUDA) ... ok
test_EmbeddingBag_empty_per_sample_weights_and_offsets_cuda_int64_float64 (__main__.TestNNDeviceTypeCUDA) ... ok
test_EmbeddingBag_per_sample_weights_and_new_offsets_cuda_int32_float16 (__main__.TestNNDeviceTypeCUDA) ... ok
test_EmbeddingBag_per_sample_weights_and_new_offsets_cuda_int32_float32 (__main__.TestNNDeviceTypeCUDA) ... ok
test_EmbeddingBag_per_sample_weights_and_new_offsets_cuda_int32_float64 (__main__.TestNNDeviceTypeCUDA) ... ok
test_EmbeddingBag_per_sample_weights_and_new_offsets_cuda_int64_float16 (__main__.TestNNDeviceTypeCUDA) ... ok
test_EmbeddingBag_per_sample_weights_and_new_offsets_cuda_int64_float32 (__main__.TestNNDeviceTypeCUDA) ... ok
test_EmbeddingBag_per_sample_weights_and_new_offsets_cuda_int64_float64 (__main__.TestNNDeviceTypeCUDA) ... ok
test_EmbeddingBag_per_sample_weights_and_no_offsets_cuda_int32_float16 (__main__.TestNNDeviceTypeCUDA) ... skipped 'Temporarily disabled. See t54369166'
test_EmbeddingBag_per_sample_weights_and_no_offsets_cuda_int32_float32 (__main__.TestNNDeviceTypeCUDA) ... skipped 'Temporarily disabled. See t54369166'
test_EmbeddingBag_per_sample_weights_and_no_offsets_cuda_int32_float64 (__main__.TestNNDeviceTypeCUDA) ... skipped 'Temporarily disabled. See t54369166'
test_EmbeddingBag_per_sample_weights_and_no_offsets_cuda_int64_float16 (__main__.TestNNDeviceTypeCUDA) ... skipped 'Temporarily disabled. See t54369166'
test_EmbeddingBag_per_sample_weights_and_no_offsets_cuda_int64_float32 (__main__.TestNNDeviceTypeCUDA) ... skipped 'Temporarily disabled. See t54369166'
test_EmbeddingBag_per_sample_weights_and_no_offsets_cuda_int64_float64 (__main__.TestNNDeviceTypeCUDA) ... skipped 'Temporarily disabled. See t54369166'
test_EmbeddingBag_per_sample_weights_and_offsets_cuda_int32_float16 (__main__.TestNNDeviceTypeCUDA) ... ok
test_EmbeddingBag_per_sample_weights_and_offsets_cuda_int32_float32 (__main__.TestNNDeviceTypeCUDA) ... ok
test_EmbeddingBag_per_sample_weights_and_offsets_cuda_int32_float64 (__main__.TestNNDeviceTypeCUDA) ... ok
test_EmbeddingBag_per_sample_weights_and_offsets_cuda_int64_float16 (__main__.TestNNDeviceTypeCUDA) ... ok
test_EmbeddingBag_per_sample_weights_and_offsets_cuda_int64_float32 (__main__.TestNNDeviceTypeCUDA) ... ok
test_EmbeddingBag_per_sample_weights_and_offsets_cuda_int64_float64 (__main__.TestNNDeviceTypeCUDA) ... ok
test_EmbeddingBag_per_sample_weights_failures_cuda_int32 (__main__.TestNNDeviceTypeCUDA) ... ok
test_EmbeddingBag_per_sample_weights_failures_cuda_int64 (__main__.TestNNDeviceTypeCUDA) ... ok
test_GroupNorm_empty_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_GroupNorm_general_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_GroupNorm_raises_error_if_one_value_per_group_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_InstanceNorm1d_general_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_InstanceNorm2d_general_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_InstanceNorm3d_general_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_LayerNorm_general_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_MarginLoss_empty_cuda_float32 (__main__.TestNNDeviceTypeCUDA) ... ok
test_MarginLoss_empty_cuda_float64 (__main__.TestNNDeviceTypeCUDA) ... ok
test_MaxPool1d_indices_cuda_bfloat16 (__main__.TestNNDeviceTypeCUDA) ... ok
test_MaxPool1d_indices_cuda_float16 (__main__.TestNNDeviceTypeCUDA) ... ok
test_MaxPool1d_indices_cuda_float32 (__main__.TestNNDeviceTypeCUDA) ... ok
test_MaxPool1d_indices_cuda_float64 (__main__.TestNNDeviceTypeCUDA) ... ok
test_MaxPool2d_indices_cuda_bfloat16 (__main__.TestNNDeviceTypeCUDA) ... ok
test_MaxPool2d_indices_cuda_float16 (__main__.TestNNDeviceTypeCUDA) ... ok
test_MaxPool2d_indices_cuda_float32 (__main__.TestNNDeviceTypeCUDA) ... ok
test_MaxPool2d_indices_cuda_float64 (__main__.TestNNDeviceTypeCUDA) ... ok
test_MaxPool3d_indices_cuda_bfloat16 (__main__.TestNNDeviceTypeCUDA) ... ok
test_MaxPool3d_indices_cuda_float16 (__main__.TestNNDeviceTypeCUDA) ... ok
test_MaxPool3d_indices_cuda_float32 (__main__.TestNNDeviceTypeCUDA) ... ok
test_MaxPool3d_indices_cuda_float64 (__main__.TestNNDeviceTypeCUDA) ... ok
test_ReflectionPad_empty_cuda_complex64 (__main__.TestNNDeviceTypeCUDA) ... ok
test_ReflectionPad_empty_cuda_float32 (__main__.TestNNDeviceTypeCUDA) ... ok
test_ReplicationPad1d_large_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_ReplicationPad2d_large_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_ReplicationPad3d_large_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_ReplicationPad_empty_cuda_complex128 (__main__.TestNNDeviceTypeCUDA) ... ok
test_ReplicationPad_empty_cuda_float64 (__main__.TestNNDeviceTypeCUDA) ... ok
test_Unfold_empty_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_activations_bfloat16_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_adaptive_avg_pool2d_output_size_one_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_adaptive_pooling_no_suppot_input_cuda_int16 (__main__.TestNNDeviceTypeCUDA) ... ok
test_adaptive_pooling_no_suppot_input_cuda_int32 (__main__.TestNNDeviceTypeCUDA) ... ok
test_adaptive_pooling_no_suppot_input_cuda_int64 (__main__.TestNNDeviceTypeCUDA) ... ok
test_adaptive_pooling_no_suppot_input_cuda_int8 (__main__.TestNNDeviceTypeCUDA) ... ok
test_adaptive_pooling_no_suppot_input_cuda_uint8 (__main__.TestNNDeviceTypeCUDA) ... ok
test_affine_2d_rotate0_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_affine_2d_rotate45_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_affine_2d_rotate90_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_affine_2d_rotateRandom_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_affine_3d_rotateRandom_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_avg_pool2d_nhwc_cuda_float16 (__main__.TestNNDeviceTypeCUDA) ... ok
test_avg_pool2d_nhwc_cuda_float32 (__main__.TestNNDeviceTypeCUDA) ... ok
test_avg_pool2d_nhwc_cuda_float64 (__main__.TestNNDeviceTypeCUDA) ... ok
test_batchnorm_eval_bfloat16_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_batchnorm_eval_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_batchnorm_grad_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_batchnorm_simple_average_cuda_float32 (__main__.TestNNDeviceTypeCUDA) ... ok
test_batchnorm_update_stats_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_clip_grad_norm_multi_device_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_contig_wrong_stride_cudnn_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_convTranspose_empty_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_conv_cudnn_mismatch_memory_format_cuda (__main__.TestNNDeviceTypeCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_conv_cudnn_nhwc_cuda_float16 (__main__.TestNNDeviceTypeCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_conv_cudnn_nhwc_cuda_float32 (__main__.TestNNDeviceTypeCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_conv_cudnn_nhwc_support_cuda_float32 (__main__.TestNNDeviceTypeCUDA) ... ok
test_conv_cudnn_nhwc_support_cuda_float64 (__main__.TestNNDeviceTypeCUDA) ... ok
test_conv_large_cuda (__main__.TestNNDeviceTypeCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_conv_large_nosplit_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_conv_noncontig_weights_and_bias_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_conv_noncontig_weights_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_conv_transposed_large_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_convert_conv2d_weight_memory_format_cuda (__main__.TestNNDeviceTypeCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_device_mask_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_elu_inplace_overlap_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_embedding_backward_cuda_float16 (__main__.TestNNDeviceTypeCUDA) ... ok
test_embedding_backward_cuda_float64 (__main__.TestNNDeviceTypeCUDA) ... ok
test_embedding_bag_bfloat16_cuda_int32 (__main__.TestNNDeviceTypeCUDA) ... ok
test_embedding_bag_bfloat16_cuda_int64 (__main__.TestNNDeviceTypeCUDA) ... ok
test_embedding_bag_device_cuda_int32_float16 (__main__.TestNNDeviceTypeCUDA) ... ok
test_embedding_bag_device_cuda_int32_float32 (__main__.TestNNDeviceTypeCUDA) ... ok
test_embedding_bag_device_cuda_int32_float64 (__main__.TestNNDeviceTypeCUDA) ... ok
test_embedding_bag_device_cuda_int64_float16 (__main__.TestNNDeviceTypeCUDA) ... ok
test_embedding_bag_device_cuda_int64_float32 (__main__.TestNNDeviceTypeCUDA) ... ok
test_embedding_bag_device_cuda_int64_float64 (__main__.TestNNDeviceTypeCUDA) ... ok
test_embedding_bag_empty_input_cuda_int32 (__main__.TestNNDeviceTypeCUDA) ... ok
test_embedding_bag_empty_input_cuda_int64 (__main__.TestNNDeviceTypeCUDA) ... ok
test_embedding_bag_non_contiguous_weight_cuda_int32_float16 (__main__.TestNNDeviceTypeCUDA) ... ok
test_embedding_bag_non_contiguous_weight_cuda_int32_float32 (__main__.TestNNDeviceTypeCUDA) ... ok
test_embedding_bag_non_contiguous_weight_cuda_int32_float64 (__main__.TestNNDeviceTypeCUDA) ... ok
test_embedding_bag_non_contiguous_weight_cuda_int64_float16 (__main__.TestNNDeviceTypeCUDA) ... ok
test_embedding_bag_non_contiguous_weight_cuda_int64_float32 (__main__.TestNNDeviceTypeCUDA) ... ok
test_embedding_bag_non_contiguous_weight_cuda_int64_float64 (__main__.TestNNDeviceTypeCUDA) ... ok
test_embedding_dense_grad_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_embedding_max_norm_device_cuda_bfloat16 (__main__.TestNNDeviceTypeCUDA) ... ok
test_embedding_max_norm_device_cuda_float16 (__main__.TestNNDeviceTypeCUDA) ... ok
test_embedding_max_norm_device_cuda_float32 (__main__.TestNNDeviceTypeCUDA) ... ok
test_embedding_max_norm_device_cuda_float64 (__main__.TestNNDeviceTypeCUDA) ... ok
test_embedding_padding_idx_cuda_bfloat16 (__main__.TestNNDeviceTypeCUDA) ... ok
test_embedding_padding_idx_cuda_float16 (__main__.TestNNDeviceTypeCUDA) ... ok
test_embedding_padding_idx_cuda_float32 (__main__.TestNNDeviceTypeCUDA) ... ok
test_embedding_padding_idx_cuda_float64 (__main__.TestNNDeviceTypeCUDA) ... ok
test_embedding_scalar_weight_error_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_empty_dropout_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_fold_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_fractional_max_pool2d_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_fractional_max_pool3d_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_fractional_max_pool_nan_inf_cuda_float16 (__main__.TestNNDeviceTypeCUDA) ... ok
test_fractional_max_pool_nan_inf_cuda_float32 (__main__.TestNNDeviceTypeCUDA) ... ok
test_fractional_max_pool_nan_inf_cuda_float64 (__main__.TestNNDeviceTypeCUDA) ... ok
test_grid_sample_2d_alert_nondeterministic_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_grid_sample_3d_alert_nondeterministic_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_grid_sample_large_cuda (__main__.TestNNDeviceTypeCUDA) ... /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3932: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  "Default grid_sample and affine_grid behavior has changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3994: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  "Default grid_sample and affine_grid behavior has changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3932: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  "Default grid_sample and affine_grid behavior has changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3994: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  "Default grid_sample and affine_grid behavior has changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3932: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  "Default grid_sample and affine_grid behavior has changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3994: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  "Default grid_sample and affine_grid behavior has changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3932: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  "Default grid_sample and affine_grid behavior has changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3994: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  "Default grid_sample and affine_grid behavior has changed "
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3932: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  "Default grid_sample and affine_grid behavior has changed "
ok
test_grid_sample_large_index_2d_cuda_float32 (__main__.TestNNDeviceTypeCUDA) ... ok
test_grid_sample_large_index_2d_cuda_float64 (__main__.TestNNDeviceTypeCUDA) ... ok
test_grid_sample_large_index_3d_cuda_float32 (__main__.TestNNDeviceTypeCUDA) ... ok
test_grid_sample_large_index_3d_cuda_float64 (__main__.TestNNDeviceTypeCUDA) ... ok
test_grid_sample_nan_inf_cuda_float32 (__main__.TestNNDeviceTypeCUDA) ... ok
test_grid_sample_nan_inf_cuda_float64 (__main__.TestNNDeviceTypeCUDA) ... ok
test_group_convTranspose_empty_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_group_conv_empty_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_gumbel_softmax_cuda_float16 (__main__.TestNNDeviceTypeCUDA) ... ok
test_gumbel_softmax_cuda_float32 (__main__.TestNNDeviceTypeCUDA) ... ok
test_gumbel_softmax_cuda_float64 (__main__.TestNNDeviceTypeCUDA) ... ok
test_hardsigmoid_grad_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_hardswish_grad_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_hardswish_inplace_overlap_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_instancenorm_raises_error_if_less_than_one_value_per_channel_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_invalid_reduction_strings_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_leaky_relu_inplace_overlap_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_linear_empty_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_log_softmax_big_cuda_float16 (__main__.TestNNDeviceTypeCUDA) ... ok
test_log_softmax_big_cuda_float32 (__main__.TestNNDeviceTypeCUDA) ... ok
test_logsigmoid_out_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_max_pool1d_corner_cases_cuda_float32 (__main__.TestNNDeviceTypeCUDA) ... skipped 'Only runs on cpu'
test_max_pool1d_corner_cases_cuda_float64 (__main__.TestNNDeviceTypeCUDA) ... skipped 'Only runs on cpu'
test_max_pool1d_cuda_float32 (__main__.TestNNDeviceTypeCUDA) ... skipped 'Only runs on cpu'
test_max_pool1d_cuda_float64 (__main__.TestNNDeviceTypeCUDA) ... skipped 'Only runs on cpu'
test_max_pool1d_errors_cuda_float32 (__main__.TestNNDeviceTypeCUDA) ... skipped 'Only runs on cpu'
test_max_pool2d_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_max_pool2d_indices_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_max_pool2d_nhwc_cuda_float16 (__main__.TestNNDeviceTypeCUDA) ... ok
test_max_pool2d_nhwc_cuda_float32 (__main__.TestNNDeviceTypeCUDA) ... ok
test_max_pool2d_nhwc_cuda_float64 (__main__.TestNNDeviceTypeCUDA) ... ok
test_max_pool_nan_inf_cuda_float16 (__main__.TestNNDeviceTypeCUDA) ... ok
test_max_pool_nan_inf_cuda_float32 (__main__.TestNNDeviceTypeCUDA) ... ok
test_max_pool_nan_inf_cuda_float64 (__main__.TestNNDeviceTypeCUDA) ... ok
test_maxpool3d_non_square_backward_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_multi_margin_loss_errors_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_multihead_attention_dtype_cuda_float16 (__main__.TestNNDeviceTypeCUDA) ... ok
test_multihead_attention_dtype_cuda_float32 (__main__.TestNNDeviceTypeCUDA) ... ok
test_multihead_attention_dtype_cuda_float64 (__main__.TestNNDeviceTypeCUDA) ... ok
test_nll_loss_empty_tensor_reduction_mean_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_nll_loss_empty_tensor_reduction_none_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_nll_loss_empty_tensor_reduction_sum_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_nll_loss_mismatched_batch_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_nll_loss_out_of_bounds_ignore_index_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_nll_loss_total_weight_is_zero_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_nn_scalars_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_nn_scalars_reductions_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_nonlinearity_propagate_nan_cuda (__main__.TestNNDeviceTypeCUDA) ... /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:1780: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:1791: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
ok
test_one_hot_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_overwrite_module_params_on_conversion_cpu_device_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_pad_cuda_complex128 (__main__.TestNNDeviceTypeCUDA) ... ok
test_pad_cuda_float64 (__main__.TestNNDeviceTypeCUDA) ... ok
test_pool3d_large_size_int64_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_pool3d_size_one_feature_dim_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_pool_invalid_size_cuda_bfloat16 (__main__.TestNNDeviceTypeCUDA) ... ok
test_pool_invalid_size_cuda_float16 (__main__.TestNNDeviceTypeCUDA) ... ok
test_pool_invalid_size_cuda_float32 (__main__.TestNNDeviceTypeCUDA) ... ok
test_pool_invalid_size_cuda_float64 (__main__.TestNNDeviceTypeCUDA) ... ok
test_pool_large_size_cuda_bfloat16 (__main__.TestNNDeviceTypeCUDA) ... ok
test_pool_large_size_cuda_float16 (__main__.TestNNDeviceTypeCUDA) ... ok
test_pool_large_size_cuda_float32 (__main__.TestNNDeviceTypeCUDA) ... ok
test_pool_large_size_cuda_float64 (__main__.TestNNDeviceTypeCUDA) ... ok
test_pooling_bfloat16_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_pooling_shape_cuda (__main__.TestNNDeviceTypeCUDA)
Test the output shape calculation for pooling functions ... ok
test_pooling_size_empty_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_pooling_zero_stride_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_prelu_backward_32bit_indexing_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_rnn_fused_cuda_float32 (__main__.TestNNDeviceTypeCUDA) ... ok
test_rnn_fused_cuda_float64 (__main__.TestNNDeviceTypeCUDA) ... ok
test_rnn_retain_variables_cuda_float16 (__main__.TestNNDeviceTypeCUDA) ... ok
test_rnn_retain_variables_cuda_float32 (__main__.TestNNDeviceTypeCUDA) ... ok
test_rnn_retain_variables_cuda_float64 (__main__.TestNNDeviceTypeCUDA) ... ok
test_silu_inplace_overlap_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_smooth_l1_loss_vs_huber_loss_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_softmax_64bit_indexing_cuda_float16 (__main__.TestNNDeviceTypeCUDA) ... ok
test_softmax_64bit_indexing_cuda_float32 (__main__.TestNNDeviceTypeCUDA) ... ok
test_softmax_bfloat16_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_softmax_cuda_float16 (__main__.TestNNDeviceTypeCUDA) ... FAIL
test_softmax_cuda_float32 (__main__.TestNNDeviceTypeCUDA) ... ok
test_softmax_results_cuda_float16 (__main__.TestNNDeviceTypeCUDA) ... ok
test_softmax_results_cuda_float32 (__main__.TestNNDeviceTypeCUDA) ... ok
test_softplus_inplace_overlap_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_softshrink_inplace_overlap_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_softshrink_negative_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_threshold_inplace_overlap_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_to_complex_cuda (__main__.TestNNDeviceTypeCUDA) ... /opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py:805: UserWarning: Complex modules are a new feature under active development whose design may change, and some modules might not work as expected when using complex tensors as parameters or buffers. Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md if a complex module does not work as expected.
  "Complex modules are a new feature under active development whose design may change, "
/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py:805: UserWarning: Complex modules are a new feature under active development whose design may change, and some modules might not work as expected when using complex tensors as parameters or buffers. Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md if a complex module does not work as expected.
  "Complex modules are a new feature under active development whose design may change, "
ok
test_triplet_margin_with_distance_loss_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_triplet_margin_with_distance_loss_default_parity_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_unfold_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_upsamplingNearest1d_launch_config_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_upsamplingNearest2d_launch_config_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_upsamplingNearest2d_launch_fail_cuda (__main__.TestNNDeviceTypeCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_upsamplingNearest2d_launch_rocm_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_upsamplingNearest3d_launch_config_cuda (__main__.TestNNDeviceTypeCUDA) ... ok
test_variable_sequence_cuda_float16 (__main__.TestNNDeviceTypeCUDA) ... ok
test_variable_sequence_cuda_float32 (__main__.TestNNDeviceTypeCUDA) ... ok
test_variable_sequence_cuda_float64 (__main__.TestNNDeviceTypeCUDA) ... ok
test_calculate_gain_leaky_relu (__main__.TestNNInit) ... ok
test_calculate_gain_leaky_relu_only_accepts_numbers (__main__.TestNNInit) ... ok
test_calculate_gain_linear (__main__.TestNNInit) ... ok
test_calculate_gain_nonlinear (__main__.TestNNInit) ... ok
test_calculate_gain_only_accepts_valid_nonlinearities (__main__.TestNNInit) ... ok
test_constant (__main__.TestNNInit) ... ok
test_deprecation (__main__.TestNNInit) ... ok
test_dirac_identity (__main__.TestNNInit) ... ok
test_dirac_only_works_on_3_4_5d_inputs (__main__.TestNNInit) ... ok
test_dirac_properties (__main__.TestNNInit) ... ok
test_eye (__main__.TestNNInit) ... ok
test_eye_only_works_on_2d_inputs (__main__.TestNNInit) ... ok
test_kaiming_normal (__main__.TestNNInit) ... ok
test_kaiming_normal_errors_on_inputs_smaller_than_2d (__main__.TestNNInit) ... ok
test_kaiming_uniform (__main__.TestNNInit) ... ok
test_kaiming_uniform_errors_on_inputs_smaller_than_2d (__main__.TestNNInit) ... ok
test_max_unpool (__main__.TestNNInit) ... ok
test_normal (__main__.TestNNInit) ... ok
test_ones_and_zeros (__main__.TestNNInit) ... ok
test_orthogonal (__main__.TestNNInit) ... ok
test_sparse_default_std (__main__.TestNNInit) ... ok
test_sparse_only_works_on_2d_inputs (__main__.TestNNInit) ... ok
test_trunc_normal (__main__.TestNNInit) ... ok
test_uniform (__main__.TestNNInit) ... ok
test_xavier_normal (__main__.TestNNInit) ... ok
test_xavier_normal_errors_on_inputs_smaller_than_2d (__main__.TestNNInit) ... ok
test_xavier_uniform (__main__.TestNNInit) ... ok
test_xavier_uniform_errors_on_inputs_smaller_than_2d (__main__.TestNNInit) ... ok

======================================================================
FAIL: test_softmax_cuda_float16 (__main__.TestNNDeviceTypeCUDA)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py", line 893, in wrapper
    method(*args, **kwargs)
  File "/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py", line 893, in wrapper
    method(*args, **kwargs)
  File "/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_device_type.py", line 287, in instantiated_test
    result = test_fn(self, *args)
  File "/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_device_type.py", line 678, in only_fn
    return fn(slf, device, *args, **kwargs)
  File "test_nn.py", line 13095, in test_softmax
    self.assertEqual(input.grad, inputf.grad.to(dtype), atol=0, rtol=0)
  File "/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py", line 1189, in assertEqual
    super().assertTrue(result, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: False is not true : Tensors failed to compare as equal!With rtol=0 and atol=0, found 1 element(s) (out of 3200) whose difference(s) exceeded the margin of error (including 0 nan comparisons). The greatest difference was 1.9073486328125e-06 (0.0030803680419921875 vs. 0.003078460693359375), which occurred at index (30, 93).

----------------------------------------------------------------------
Ran 2165 tests in 745.407s

FAILED (failures=1, skipped=109, expected failures=4)
