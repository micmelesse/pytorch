// !!! This is a file automatically generated by hipify!!!
#include <gtest/gtest.h>

#include <ATen/hip/HIPContext.h>
#include <c10/hip/impl/HIPGuardImpl.h>
#include <c10/core/impl/InlineEvent.h>
#include <c10/core/Event.h>
#include <ATen/hip/impl/HIPGuardImplMasqueradingAsCUDA.h>
#include <ATen/hip/HIPMultiStreamGuard.h>
#include <ATen/hip/HIPEvent.h>

#include <hip/hip_runtime.h>

#include <functional>
#include <thread>
#include <unordered_set>

#define ASSERT_EQ_CUDA(X, Y) \
  {                          \
    bool isTRUE = X == Y;    \
    ASSERT_TRUE(isTRUE);     \
  }

#define ASSERT_NE_CUDA(X, Y) \
  {                          \
    bool isFALSE = X == Y;   \
    ASSERT_FALSE(isFALSE);   \
  }

/*
   Tests related to ATen streams.
   */
// Verifies streams are live through copying and moving
TEST(TestStream, CopyAndMoveTest) {
  if (!at::cuda::is_available()) return;
  int32_t device = -1;
  hipStream_t cuda_stream;

  // Tests that copying works as expected and preserves the stream
  at::hip::HIPStreamMasqueradingAsCUDA copyStream = at::hip::getStreamFromPoolMasqueradingAsCUDA();
  {
    auto s = at::hip::getStreamFromPoolMasqueradingAsCUDA();
    device = s.device_index();
    cuda_stream = s.stream();

    copyStream = s;

    ASSERT_EQ_CUDA(copyStream.device_index(), device);
    ASSERT_EQ_CUDA(copyStream.stream(), cuda_stream);
  }

  ASSERT_EQ_CUDA(copyStream.device_index(), device);
  ASSERT_EQ_CUDA(copyStream.stream(), cuda_stream);

  // Tests that moving works as expected and preserves the stream
  at::hip::HIPStreamMasqueradingAsCUDA moveStream = at::hip::getStreamFromPoolMasqueradingAsCUDA();
  {
    auto s = at::hip::getStreamFromPoolMasqueradingAsCUDA();
    device = s.device_index();
    cuda_stream = s.stream();

    moveStream = std::move(s);

    ASSERT_EQ_CUDA(moveStream.device_index(), device);
    ASSERT_EQ_CUDA(moveStream.stream(), cuda_stream);
  }

  ASSERT_EQ_CUDA(moveStream.device_index(), device);
  ASSERT_EQ_CUDA(moveStream.stream(), cuda_stream);
}

// Verifies streams are set properly
TEST(TestStream, GetAndSetTest) {
  if (!at::cuda::is_available()) return;
  at::hip::HIPStreamMasqueradingAsCUDA myStream = at::hip::getStreamFromPoolMasqueradingAsCUDA();

  // Sets and gets
  at::hip::setCurrentHIPStreamMasqueradingAsCUDA(myStream);
  at::hip::HIPStreamMasqueradingAsCUDA curStream = at::hip::getCurrentHIPStreamMasqueradingAsCUDA();

  ASSERT_EQ_CUDA(myStream, curStream);

  // Gets, sets, and gets default stream
  at::hip::HIPStreamMasqueradingAsCUDA defaultStream = at::hip::getDefaultHIPStreamMasqueradingAsCUDA();
  at::hip::setCurrentHIPStreamMasqueradingAsCUDA(defaultStream);
  curStream = at::hip::getCurrentHIPStreamMasqueradingAsCUDA();

  ASSERT_NE_CUDA(defaultStream, myStream);
  ASSERT_EQ_CUDA(curStream, defaultStream);
}

void thread_fun(at::optional<at::hip::HIPStreamMasqueradingAsCUDA>& cur_thread_stream) {
  auto new_stream = at::hip::getStreamFromPoolMasqueradingAsCUDA();
  at::hip::setCurrentHIPStreamMasqueradingAsCUDA(new_stream);
  cur_thread_stream = {at::hip::getCurrentHIPStreamMasqueradingAsCUDA()};
  ASSERT_EQ_CUDA(*cur_thread_stream, new_stream);
}

// Ensures streams are thread local
TEST(TestStream, MultithreadGetAndSetTest) {
  if (!at::cuda::is_available()) return;
  at::optional<at::hip::HIPStreamMasqueradingAsCUDA> s0, s1;

  std::thread t0{thread_fun, std::ref(s0)};
  std::thread t1{thread_fun, std::ref(s1)};
  t0.join();
  t1.join();

  at::hip::HIPStreamMasqueradingAsCUDA cur_stream = at::hip::getCurrentHIPStreamMasqueradingAsCUDA();
  at::hip::HIPStreamMasqueradingAsCUDA default_stream = at::hip::getDefaultHIPStreamMasqueradingAsCUDA();

  ASSERT_EQ_CUDA(cur_stream, default_stream);
  ASSERT_NE_CUDA(cur_stream, *s0);
  ASSERT_NE_CUDA(cur_stream, *s1);
  ASSERT_NE_CUDA(s0, s1);
}

// CUDA Guard
TEST(TestStream, CUDAGuardTest) {
  if (!at::cuda::is_available()) return;
  if (at::cuda::getNumGPUs() < 2) {
    return;
  }

  // -- begin setup

  ASSERT_EQ_CUDA(at::hip::current_device(), 0);
  std::vector<at::hip::HIPStreamMasqueradingAsCUDA> streams0 = {
      at::hip::getDefaultHIPStreamMasqueradingAsCUDA(), at::hip::getStreamFromPoolMasqueradingAsCUDA()};
  ASSERT_EQ_CUDA(streams0[0].device_index(), 0);
  ASSERT_EQ_CUDA(streams0[1].device_index(), 0);
  at::hip::setCurrentHIPStreamMasqueradingAsCUDA(streams0[0]);

  std::vector<at::hip::HIPStreamMasqueradingAsCUDA> streams1;
  {
    at::hip::HIPGuardMasqueradingAsCUDA device_guard(1);
    streams1.push_back(at::hip::getDefaultHIPStreamMasqueradingAsCUDA());
    streams1.push_back(at::hip::getStreamFromPoolMasqueradingAsCUDA());
  }
  ASSERT_EQ_CUDA(streams1[0].device_index(), 1);
  ASSERT_EQ_CUDA(streams1[1].device_index(), 1);
  at::hip::setCurrentHIPStreamMasqueradingAsCUDA(streams1[0]);

  ASSERT_EQ_CUDA(at::hip::current_device(), 0);

  // -- end setup

  // Test that all original streams are recorded.
  {
    at::cuda::CUDAMultiStreamGuard guard;
    ASSERT_EQ_CUDA(guard.original_streams().size(), at::cuda::getNumGPUs());
    ASSERT_EQ_CUDA(guard.original_streams()[0], streams0[0]);
    ASSERT_EQ_CUDA(guard.original_streams()[1], streams1[0]);
  }

  // Setting a stream changes the current device and the stream on that device
  {
    at::hip::HIPStreamGuardMasqueradingAsCUDA guard(streams1[1]);
    ASSERT_EQ_CUDA(guard.current_device(), at::Device(at::kCUDA, 1));
    ASSERT_EQ_CUDA(at::hip::current_device(), 1);
    ASSERT_EQ_CUDA(at::hip::getCurrentHIPStreamMasqueradingAsCUDA(1), streams1[1]);
  }

  // Device and stream are now reset
  ASSERT_EQ_CUDA(at::hip::current_device(), 0);
  ASSERT_EQ_CUDA(at::hip::getCurrentHIPStreamMasqueradingAsCUDA(1), streams1[0]);

  // Setting only the device changes only the current device and not the stream
  {
    at::hip::HIPGuardMasqueradingAsCUDA guard(/*device=*/1);
    ASSERT_EQ_CUDA(guard.current_device(), at::Device(at::kCUDA, 1));
    ASSERT_EQ_CUDA(at::hip::current_device(), 1);
    ASSERT_EQ_CUDA(at::hip::getCurrentHIPStreamMasqueradingAsCUDA(1), streams1[0]);
  }

  ASSERT_EQ_CUDA(at::hip::current_device(), 0);
  ASSERT_EQ_CUDA(at::hip::getCurrentHIPStreamMasqueradingAsCUDA(0), streams0[0]);
}

// Streampool Round Robin
TEST(TestStream, StreamPoolTest) {
  if (!at::cuda::is_available()) return;
  std::vector<at::hip::HIPStreamMasqueradingAsCUDA> streams{};
  for (int i = 0; i < 200; ++i) {
    streams.emplace_back(at::hip::getStreamFromPoolMasqueradingAsCUDA());
  }

  std::unordered_set<hipStream_t> stream_set{};
  bool hasDuplicates = false;
  for (auto i = decltype(streams.size()){0}; i < streams.size(); ++i) {
    hipStream_t cuda_stream = streams[i];
    auto result_pair = stream_set.insert(cuda_stream);
    if (!result_pair.second)
      hasDuplicates = true;
  }

  ASSERT_TRUE(hasDuplicates);
}

// Multi-GPU
TEST(TestStream, MultiGPUTest) {
  if (!at::cuda::is_available()) return;
  if (at::cuda::getNumGPUs() < 2)
    return;

  at::hip::HIPStreamMasqueradingAsCUDA s0 = at::hip::getStreamFromPoolMasqueradingAsCUDA(true, 0);
  at::hip::HIPStreamMasqueradingAsCUDA s1 = at::hip::getStreamFromPoolMasqueradingAsCUDA(false, 1);

  at::hip::setCurrentHIPStreamMasqueradingAsCUDA(s0);
  at::hip::setCurrentHIPStreamMasqueradingAsCUDA(s1);

  ASSERT_EQ_CUDA(s0, at::hip::getCurrentHIPStreamMasqueradingAsCUDA());

  at::hip::HIPGuardMasqueradingAsCUDA device_guard{1};
  ASSERT_EQ_CUDA(s1, at::hip::getCurrentHIPStreamMasqueradingAsCUDA());
}

// CUDAEvent Syncs
TEST(TestStream, CUDAEventSyncTest) {
  if (!at::cuda::is_available()) return;
  const auto stream = at::hip::getStreamFromPoolMasqueradingAsCUDA();
  at::cuda::CUDAEvent event;

  ASSERT_TRUE(event.query());

  event.recordOnce(stream);

  const auto wait_stream0 = at::hip::getStreamFromPoolMasqueradingAsCUDA();
  const auto wait_stream1 = at::hip::getStreamFromPoolMasqueradingAsCUDA();

  event.block(wait_stream0);
  event.block(wait_stream1);

  hipStreamSynchronize(wait_stream0);
  ASSERT_TRUE(event.query());
}

// Cross-Device Events
TEST(TestStream, CrossDeviceTest) {
  if (!at::cuda::is_available()) return;
  if (at::cuda::getNumGPUs() < 2)
    return;

  const auto stream0 = at::hip::getStreamFromPoolMasqueradingAsCUDA();
  at::cuda::CUDAEvent event0;

  at::hip::set_device(1);
  const auto stream1 = at::hip::getStreamFromPoolMasqueradingAsCUDA();
  at::cuda::CUDAEvent event1;

  event0.record(stream0);
  event1.record(stream1);

  event0 = std::move(event1);

  ASSERT_EQ_CUDA(event0.device(), at::Device(at::kCUDA, 1));

  event0.block(stream0);

  hipStreamSynchronize(stream0);
  ASSERT_TRUE(event0.query());
}

// Generic Events
TEST(TestStream, GenericInlineCUDAEventTest) {
  if (!at::cuda::is_available()) return;

  c10::impl::InlineEvent<c10::hip::impl::CUDAGuardImpl> event{c10::DeviceType::CUDA};
  c10::Stream stream = at::hip::getStreamFromPoolMasqueradingAsCUDA();

  event.record(stream);

  const c10::Stream wait_stream0 = at::hip::getStreamFromPoolMasqueradingAsCUDA();
  const c10::Stream wait_stream1 = at::hip::getStreamFromPoolMasqueradingAsCUDA();

  event.block(wait_stream0);
  event.block(wait_stream1);

  const at::hip::HIPStreamMasqueradingAsCUDA cuda_stream{wait_stream0};
  hipStreamSynchronize(cuda_stream);

  ASSERT_TRUE(event.query());
}

TEST(TestStream, GenericVirtualCUDAEventTest) {
  if (!at::cuda::is_available()) return;

  c10::Event event{c10::DeviceType::CUDA};
  c10::Stream stream = at::hip::getStreamFromPoolMasqueradingAsCUDA();

  event.recordOnce(stream);

  const c10::Stream wait_stream0 = at::hip::getStreamFromPoolMasqueradingAsCUDA();
  const c10::Stream wait_stream1 = at::hip::getStreamFromPoolMasqueradingAsCUDA();

  wait_stream0.wait(event);
  wait_stream1.wait(event);

  const at::hip::HIPStreamMasqueradingAsCUDA cuda_stream{wait_stream0};
  hipStreamSynchronize(cuda_stream);

  ASSERT_TRUE(event.query());
  ASSERT_TRUE(event.flag() == c10::EventFlag::PYTORCH_DEFAULT);
}
